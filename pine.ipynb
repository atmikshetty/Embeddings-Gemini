{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gemini Pro and  Pinecone Database to Store Research Paper Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access all the API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the research paper\n",
    "def read_data(directory):\n",
    "    loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2024-5-7\\nAdvancing Multimodal Medical Capabilities of\\nGemini\\nGoogle Research and Google DeepMind†\\nMany clinical tasks require an understanding of specialized data, such as medical images and genomics,\\nwhich is not typically found in general-purpose large multimodal models. Building upon Gemini’s\\nmultimodal models, we develop several models within the new Med-Gemini family that inherit core\\ncapabilities of Gemini and are optimized for medical use via fine-tuning with 2D and 3D radiology,\\nhistopathology, ophthalmology, dermatology and genomic data. Med-Gemini-2D sets a new standard for\\nAI-based chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results\\nacross two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of AI reports\\non normal cases, and 43% and 65% on abnormal cases, are evaluated as “equivalent or better” than\\nthe original radiologists’ reports. We demonstrate the first ever large multimodal model-based report\\ngeneration for 3D computed tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports\\nconsidered clinically acceptable, although additional research is needed to meet expert radiologist\\nreporting quality. Beyond report generation, Med-Gemini-2D surpasses the previous best performance\\nin CXR visual question answering (VQA) and performs well in CXR classification and radiology VQA,\\nexceeding SoTA or baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology\\nimage classification, Med-Gemini-2D surpasses baselines across 18 out of 20 tasks and approaches\\ntask-specific model performance. Beyond imaging, Med-Gemini-Polygenic outperforms the standard\\nlinear polygenic risk score-based approach for disease risk prediction and generalizes to genetically\\ncorrelated diseases for which it has never been trained. Although further development and evaluation\\nare necessary in the safety-critical medical domain, our results highlight the potential of Med-Gemini\\nacross a wide range of medical tasks.\\n†See Contributions and Acknowledgments section for full author list.\\nCorresponding authors: {dangolden,shekazizi,kellych,roryp}@google.com.\\n©2024 Google. All rights reservedarXiv:2405.03162v1  [cs.CV]  6 May 2024', metadata={'source': 'data\\\\med.pdf', 'page': 0}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\n1. Introduction\\nMedicaldatafromdiversesourceslikebiobanks,electronichealthrecords,medicalimaging,wearables,\\nbiosensors, and genomic sequencing are enabling the development of multimodal AI solutions that can\\nbetter capture the complexity of human health and disease (Acosta et al., 2022). While AI in medicine\\nhas primarily focused on narrow tasks with single input and output types (Rajpurkar et al., 2022),\\nrecent advances in generative AI show promise in addressing multimodal, multi-task challenges in\\nmedical settings (Moor et al., 2023a,b).\\nThe emergence of large language models (LLMs) and large multimodal models (LMMs) such\\nas Flamingo (Alayrac et al., 2022), PaLI (Chen et al., 2022), GPT-4 (Achiam et al., 2023), GPT-4v\\n(OpenAI, 2023), PaLM (Anil et al., 2023; Chowdhery et al., 2023), LLaMA (Touvron et al., 2023),\\nLLaVa(Liuetal.,2023,2024a), andMistral7B(Jiangetal.,2023)thatpromisesignificantlyenhanced\\ncontext length and improved multimodal capabilities suggests that the realization of highly complex\\nmultimodal reasoning across various medical data will soon be achievable. These advancements have\\ncatalyzed the expansion of LLMs specifically designed for medical domains, such as Med-PaLM and its\\nsuccessor Med-PaLM 2 (Singhal et al., 2023a,b), Clinical Camel (Toma et al., 2023), MedAlpaca (Han\\net al., 2023), BioMistral (Labrak et al., 2024), sc-GPT (Cui et al., 2024), and others. Going beyond\\ntext alone, recent works have extended the capabilities of these base multimodal models by building\\nmodels that cover various medical imaging modalities like Med-PaLM M (Tu et al., 2024), Med-\\nFlamingo (Moor et al., 2023b) as well as those that focus on a specific imaging domain, such as\\nradiology (Hamamci et al., 2024; Hyland et al., 2023; Tanno et al., 2024; Thawkar et al., 2023; Xu\\net al., 2023) and histopathology (Ikezogwo et al., 2024; Lu et al., 2024; Sun et al., 2024).\\nThereleaseoftheGeminimodels(GeminiTeam,Google,2023;Google,2024),withtheiradvanced\\nmultimodal capabilities and breakthroughs in long-context understanding, marked a significant step\\nforward in multimodal reasoning. Given its inherent human focus, medicine is a field in which\\nadvanced multimodal systems like Gemini are expected to be transformative (Acosta et al., 2022).\\nEvaluations have already started to evaluate the base performance of these newer multimodal models\\n(Pal and Sankarasubbu, 2024). However, the true potential of multimodal foundation models in the\\nmedical field remains largely underexplored due to the complexity of optimizing for problems in this\\nfield (Moor et al., 2023a; Rajpurkar et al., 2022) and a lack of diverse and meaningful evaluations\\nthat are grounded in clinical use cases (Fleming et al., 2023; Royer et al., 2024; Zhang et al., 2023a).\\nTo better understand the nuances of model capabilities and limitations, it is necessary to optimize\\nmultimodal models for a diversity of relevant clinical applications and rigorously evaluate them on\\nappropriate clinical datasets.\\nThis report details our efforts in exploring Gemini’s capabilities across a range of challenging multi-\\nmodal medical tasks. Our evaluation benchmarks include 2D and 3D radiology images, histopathology\\npatches, ophthalmology images, dermatology images, and genetic risk scoring. Our benchmark suite\\nincludes both open benchmark datasets and our own curated datasets. Open benchmark datasets have\\nthe advantage of being established and enabling direct comparison to others’ work, but they are often\\nlimited or methodologically flawed, leading to results that can overstate performance. For the custom\\nbenchmarks that we introduce, we have prioritized high quality metrics that are closely correlated to\\nclinical utility. In particular, we focused on expert human evaluations for quantifying performance on\\nCXR and CT report generation and on open visual question answering (VQA) questions from VQA-Rad.\\nAdditionally, we compared Med-Gemini to previous work or to the non-medically tuned version of\\nGemini where possible.\\nWhere we believed it was helpful, we have proactively improved the quality of certain open\\nbenchmarks. This includes updating and correcting erroneous labels (such as MIMIC-CXR-JPG\\nclassification labels), extending the task scope of datasets (such as introducing VQA question/answer\\npairs for MIMIC-CXR), and refining data splits to remove train-test contamination (such as PAD-UFES-\\n20 and VQA-Rad). We hope to release these improvements publicly soon.\\n2', metadata={'source': 'data\\\\med.pdf', 'page': 1}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\n2D \\n3D \\nGene \\nVisual Question \\nAnswering \\nPolygenic Risk \\nPrediction 2D Report \\nGeneration \\n3D Report \\nGeneration Classiﬁcation \\nMed-Gemini \\nGemini Medical Modalities \\n& Knowledge \\nApplications \\nVQA-Rad CXRVQA-Rad CXR\\nMIMIC-CXR VQASlake-VQAPathVQA\\nMIMIC-CXR Report IND1 Report\\nMIMIC-CXR Report\\nMajor depression\\nRheumatoid arthritisAllcause mortalityCOPD\\nAtrial fibrillation\\nCoronary artery diseaseStroke\\nPneumonia\\nHypertensionAsthma\\nType 2 diabetes\\nHypercholesterolemiaDiabetic retinopathyGlaucoma\\nHemorrhagePRP Scars\\nHard ExudatesMIMIC-CXR\\nPathology PatchDR Lesion\\nPAD-UFES-20ChestX-ray14CheXpert20\\n10\\n01020304050Relative Improvement over Best-in-class (%)\\nQuestion\\nAnsweringReport\\nGenerationGenomic\\nDisease Prediction ClassificationClassification\\n() Signifies expert evaluation, while others are assessed using automated metrics such as RadGraph-F1, Accuracy, AUC, and F1 scores.\\n() Indicates comparison with Gemini where best-in-class performance is not available.\\nFigure 1|Overview of our approach to curate and assess our family of medically tuned Gemini models,\\nMed-Gemini. (top) These models build upon Gemini’s powerful capabilities in advanced reasoning, multimodal\\nunderstanding, and long-context processing enriched with patient representation and medical knowledge.\\n(bottom) Relative performance of Med-Gemini compared to SoTA or baselines across various tasks as detailed\\nin Table A.15. Expert evaluation confirms that Med-Gemini-2D sets a new standard for AI-powered chest X-ray\\nreport generation, with relative improvements of 10% and 18% over the previous leading model across two\\ndistinct datasets. In histopathology, ophthalmology, and dermatology image classification, it surpasses baseline\\non 18 out of 20 tasks and approaches task-specific model performance. Med-Gemini-Polygenic outperforms the\\nstandard approach for disease risk prediction and generalizes to diseases for which it has never been trained.\\n3', metadata={'source': 'data\\\\med.pdf', 'page': 2}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nIn this report, we expand the fine-tuned family of models, Med-Gemini, specifically focusing on\\nmedical imaging and genomics. The models described here were tuned on a dataset of 7 million\\nsamples obtained from 3.7 million medical images and cases, spanning medical image classification,\\nVQA, report generation, and genomic risk prediction, detailed in Section 2. Importantly, this dataset\\nincludes mostly free text paired with medical data, which eliminates the need for expensive expert\\nlabeling of the training data. We intentionally explore both medical image-based tasks and also\\nthe non-image-based task of polygenic risk prediction in order to evaluate the potential of Med-\\nGemini beyond imaging and in the crucial medical domain of long term risk prediction. Our findings\\ndemonstrate that LMMs have significantly advanced over the past year and are able to perform an\\nincreasing range of challenging tasks. Our key contributions are summarized as follows:\\n•Med-Gemini : A family of generalist medical AI models fine-tuned from Gemini, capable of\\nperforming a diverse set of medical tasks including medical image classification, VQA, report\\ngeneration, and genomic risk prediction. Med-Gemini extends Gemini’s capabilities to include\\ninterpretation of diverse medical data, including both genomics and 2D and 3D medical images.\\nAdditional capabilities of Med-Gemini are described in “Capabilities of Gemini Models in Medicine”\\nby Saab et al. (2024).\\n•Clinically-relevant benchmarking : We evaluate Gemini and Med-Gemini on a comprehensive set\\nof clinically relevant benchmarks including 22 datasets across five different tasks and six distinct\\nmedical image modalities. Our evaluation suite includes eight out-of-distribution datasets to\\nassess generalization capabilities of this new family of models. Our assessments primarily consist\\nof automated metrics but we rely on expert human evaluation for tasks where expert human\\njudgment is critical, namely chest X-ray and computed tomography (CT) report generation and\\nradiology VQA on open questions in the VQA-Rad dataset.\\n•Promising or best-in-class performance in several clinically relevant tasks : Med-Gemini\\ndemonstrates best in class performance on chest X-ray and CT report generation and chest X-ray\\nclassification. Med-Gemini can also be used to predict disease and mortality risk more accurately\\nthan a standard linear polygenic risk score (PRS) based approach. Med-Gemini approaches the\\nperformance of models trained using orders of magnitude more training examples on dermatology,\\nhistopathology,andophthalmologyimageclassificationanddemonstratescompetitiveperformance\\nacross several VQA tasks across pathology and radiology.\\n2. Datasets\\nMany different public and private datasets were used in the training and evaluation of Med-Gemini.\\nAll datasets were de-identified. Open datasets were used in accordance with their existing licenses\\nand private datasets were used with permission and appropriate licenses.\\n2.1. Datasets for fine-tuning and instruction-tuning\\nDatasets were split into train, validation, and test sets by patient identifier when available. When\\npatient identifiers were not available for a dataset, we ensured that there was no case or image overlap\\nbetween splits.\\n2.1.1. Public datasets\\nMIMIC-CXR: MIMIC-CXR contains 377,110 images from 65,379 patients, with de-identified free-text\\nreports describing the images (Goldberger et al., 2000; Johnson et al., 2019a,c). This dataset is the\\nlargest public chest X-ray dataset, acquired in the emergency department of Beth Israel Deaconess\\nMedical Center in the US. For each patient, there are multiple views and a corresponding report\\nlabeled for 13 common radiological conditions using the CheXpert labeler (Irvin et al., 2019) or with\\n4', metadata={'source': 'data\\\\med.pdf', 'page': 3}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\n“no finding” if no condition is present. Available labels include atelectasis, cardiomegaly, consolidation,\\nedema, enlarged cardiomediastinum, fracture, lung lesion, lung opacity, pleural effusion, pleural\\nother, pneumonia, pneumothorax, support devices, and no finding. We used the MIMIC-CXR training\\nset (237,912 images) to fine-tune Gemini as described in Section 3 and detailed in Table 1. We\\nfurther employed the test cases of MIMIC-CXR as a benchmark for multiple evaluation tasks including\\nclassification,reportgenerationandVQA.Forthereportgenerationtask,weusedthechestX-rayimage\\ncorresponding to the frontal view (anterior-posterior or posterior-anterior) to generate the Findings\\nandImpression sections, similar to prior works (Tanno et al., 2024). For cases where no frontal view\\nwas available, we excluded them from our evaluation. For the VQA task, we utilized the condition-\\ndependent VQA dataset (e.g. pleural effusion presence/location/severity) introduced in Xu et al.\\n(2023), which we will make publicly available soon. In addition, we have used radiologist-adjudicated\\nupdated labels for findings that are also planned to be released soon (Park et al., 2024).\\nMendeley digital knee X-ray images: This dataset consists of digital X-ray images of knee joints\\nwhich were collected from hospitals and diagnostic centers. Original images are 8-bit grayscale.\\nEach knee X-ray image was manually annotated by two medical experts following the Kellgren and\\nLawrence system for classification of osteoarthritis (Gornale and Patravali, 2020). There are a total of\\n1,633 unique images in this dataset, and we utilized 1,469 images in our training.\\nPAD-UFES-20: The dataset consists of 1,373 patients, 1,641 skin lesions, and 2,298 images. Skin\\nlesion images were collected from various smartphones and exhibit variations in resolution, size,\\nand lighting conditions. The dataset was acquired in collaboration with the Dermatological and\\nSurgical Assistance Program (PAD) at the Federal University of Espírito Santo (UFES-Brazil) (Pacheco\\net al., 2020). PAD is a non-profit program offering free skin lesion treatment, particularly to those\\nwho cannot afford private care. The dataset includes images of six different skin lesion types and\\ndiagnostics: three skin diseases and three skin cancers. These include basal cell carcinoma (BCC),\\nsquamous cell carcinoma (SCC), actinic keratosis (ACK), seborrheic keratosis (SEK), Bowen’s disease\\n(BOD), melanoma (MEL), and nevus (NEV). We randomly split the dataset into train (corresponding\\nto 90% of total samples) and test (10% of total samples). We utilized 2,047 training samples from\\nthis dataset in our training corpus when fine-tuning Gemini as described in Section 3 and detailed in\\nTable 1. We intend to publicly release our dataset split soon.\\nNational Lung Screening Trial (NLST): We utilized CT volumes from the validation subset of the\\nNLST dataset (NLST, 2014) and processed them through a lung cancer screening system (Kiraly\\net al., 2024) to create a dataset of the most salient 2D slices from CT volumes. Captions were assigned\\nbased on the scr_group label in the NLST participant dictionary: values of 1 and 2 were considered\\nas having nodules whereas a value of 3 were considered as not having a nodule. We then selected a\\ntotal of 2,199 slices consisting of 1,324 studies with nodules and 875 without nodules for further\\nanalysis from the previously defined validation set (Ardila et al., 2019). The lung cancer screening\\nsystem generated captions for each slice based on the first detected region. Slices without nodules\\nreceived simpler descriptions, for example: “An axial CT slice of the middle lungs with no nodules.” For\\nslices containing nodules, the captions included: location of the nodule (left or right lung), suspicion\\nlevel for malignancy, and estimated size in millimeters based on the screening system’s output. We\\nthen split the slices into training and validation evenly based on the presence of nodules, allocating\\n80% for training, including 1,759 image and caption pairs, and 20% for validation, including 440\\nimage and caption pairs. All 2D slice images were set to a window of [−1400𝐻𝑈,100𝐻𝑈].\\nSlake-VQA: Thisdatasetisalargebilingual(EnglishandChinese)VQAdatasetmeticulouslyannotated\\nby experienced physicians (Liu et al., 2021). It offers 642 images with 14,028 question-answer pairs in\\nthree imaging modalities (i.e. CXR, CT, MRI). Slake-VQA includes various areas of radiology, covering\\nhuman body regions like the brain, neck, chest, abdomen, and pelvic cavity. The dataset comprises\\n9,849 VQA samples for training, 2,109 for validation, and 2,070 for testing. Questions are diverse,\\n5', metadata={'source': 'data\\\\med.pdf', 'page': 4}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nincluding both open-ended (free-form) and closed-ended (yes/no) formats. They probe various image\\naspects such as plane, quality, position, organ, abnormality, size, color, shape, and related medical\\nknowledge. We used only English-language examples from the official splits, which included 4,919\\ntraining, 1,053 validation, and 1,061 test examples.\\nPathVQA: This is a dataset of question-answer pairs on pathology images (He et al., 2020). The\\ndataset includes both open-ended questions and closed-ended (yes/no) questions and is built with\\nautomated methods using two publicly-available pathology textbooks and a publicly-available digital\\nlibrary. The dataset includes 32,632 question-answer pairs on 4,289 images. The official training,\\nvalidation, and test splits contain 19,654, 6,259, and 6,719 QA pairs. We leveraged the official train\\nand test sets for training our model and evaluating its performance, respectively.\\nVQA-Med: The VQA-Med-2019 dataset offers a collection of medical images and associated question-\\nanswer (QA) pairs for model training and evaluation (Ben Abacha et al., 2021). It includes a training\\nset with 3,200 medical images and 12,792 QA pairs, a validation set with 500 medical images and\\n2,000 QA pairs, and a test set containing 500 medical images and 500 questions. For the purpose\\nof this report, we removed all images that overlapped with VQA-Rad (Lau et al., 2018) to avoid\\ncontamination. This resulted in 12,664 QA pairs used in training.\\nUK Biobank: Genetic factors play a significant role in an individual’s risk of developing various\\ndiseases. In this work, we used UK Biobank (Bycroft et al., 2018), a resource of nearly 500,000\\nde-identified individuals with genetic, lifestyle, and health information, to develop a task that takes\\nas input an embedding of an individual’s genomic data and uses it to predict an individual’s status\\nfor various broad health outcomes. We extracted a set of 432,090 samples of European genetically\\ninferred ancestry with genomic data passing quality control thresholds and split it randomly into\\ntrain, validation, and test splits containing 60%, 20%, and 20% of the samples, respectively. Following\\nbest practices for polygenic risk prediction, we avoided including individuals who were genetically\\nsimilar in two different data splits (Choi et al., 2020).\\nPMC-OA: PMC-OA is a medical dataset with image-caption pairs collected from PubMedCentral’s\\nOpenAccess subset. Using the method described in Zhang et al. (2023b), we retrieved 3,110,109\\nscientific papers containing 15,505,259 image-caption pairs. To ensure meaningful analysis, we\\nfiltered for image-caption pairs containing at least one photographic image (e.g., excluding images\\ncorresponding to data figures), resulting in a final dataset of 2,246,656 image-caption pairs.\\n2.1.2. Private datasets\\nHistopathology patches: Pathological examination of tissue samples is crucial for effective diagnosis\\nand treatment planning. Data from nine tasks across six tissue types from prior work (Lai et al.,\\n2023) were used in our training set (Table A.8). Multi-class annotation masks were used for both\\nsampling image patches from whole-slide images as well as generating captions. Patches of size\\n256×256 were sampled from whole slide images in a class-balanced manner. For each of the\\nnine tasks, up to 10,000 image patches were sampled for three different magnification levels (2, 1,\\nand 0.5 microns-per-pixel), resulting in 207,603 unique patches. Patch-level captions were created\\nvia prompting of a large language model (Gemini Pro) with inputs including structured slide-level\\nmetadata as well as patch-level annotation labels. Multiple captions per class for each task were\\ngenerated and then manually reviewed to ensure an appropriate level of detail and accuracy, resulting\\nin 5–7 captions per class across tasks. Combining the sampled patches with our curated captions\\nresulted in 1,550,976 image-text pairs for fine-tuning. For examples of some of our curated captions\\ncorresponding to the annotation labels, see Table A.7.\\n6', metadata={'source': 'data\\\\med.pdf', 'page': 5}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nFundus images (EyePACS): Diabetic retinopathy is the leading cause of blindness in the working-age\\npopulation of the developed world. We used the de-identified dataset from EyePACS Inc. (Cuadros\\nand Bresnick, 2009) and converted diabetic lesion-level presence labels to captions. The lesions\\nconsidered were microaneurysms, hemorrhages, hard exudates, panretinal photocoagulation (PRP)\\nscars, neovascularization of the disc and neovascularization elsewhere. For caption conversion, if\\na given image has lesion presence, for example, microaneurysm and hemorrhage, the associated\\ngenerated caption was “microaneurysm is present, hemorrhage is present.” For healthy eyes, we use\\n“no diabetic retinopathy related lesion” as the caption. 12,976 images with lesions and 3,000 healthy\\neye images were used to construct the dataset.\\nComputed tomography images (CT-US1): A comprehensive dataset comprising 753,247 CT studies\\nwith associated radiology reports from 615,384 patients was obtained from three major hospital\\nregions in the United States. These CT studies included head/neck, chest, heart, abdominal, spine,\\nand extremity regions imaged with and without contrast. To ensure robust evaluation, we employed\\na patient-level random split for training, validation, and testing. The data was divided into 70% for\\ntraining, 15% for validation, and 15% for testing on the patient level. After an ingestion process this\\nresulted in 657,719 training volumes and a total of 23,649 validation volumes. Due to the reliance\\non expert evaluation for report generation, a subset of 92 non-contrast head/neck CT volumes from\\nunique patients in the test set was used for model assessment. Volumes were prepared as described in\\nSection 2.3. Only axial image volumes containing more than 10 slices were included in the prepared\\ndata and the volume within the study with the most axial slices was selected for inference.\\nWe also carefully processed the existing dataset to create an extra 2D CT slice dataset specifically\\ntailoredfortrainingour2Dmodel. Thisinvolvedfilteringradiologyreportsforspecificseriesandimage\\nnumbers, selecting the correct images, and windowing them to a window of [−1000𝐻𝑈,100𝐻𝑈]. To\\nensure that the text pertained directly to the CT slice in question and was a comprehensive description\\nof it, captions were generated by combining the sentence of the report referencing the image along\\nwith the following sentence. This process resulted in a dataset of 4,009 images consisting of 3,207\\ntraining and 802 validation examples, primarily focused on CT studies of the abdomen and pelvis.\\nChest X-ray images (CXR-US2): The CXR-US2 dataset corresponds to the training set of US1 in Xu\\net al. (2023). This dataset consists of 132,680 frontal chest X-ray images from 12,988 patients taken\\nat an academic medical center in Illinois, USA. Further descriptive statistics can be found in Xu et al.\\n(2023).\\n2.2. Held-out datasets for evaluation and benchmarking\\nBeyond the test sets associated with our training datasets (described above), we also utilized multiple\\nheld-out and out-of-distribution (OOD) datasets.\\n2.2.1. Public datasets\\nCheXpert The CheXpert dataset is similar to the MIMIC-CXR dataset and consists of 224,316 chest\\nX-ray images (both frontal and lateral views) from 65,240 patients (Irvin et al., 2019). It labels 14\\ndistinct thoracic conditions, including “No Finding”. The original CheXpert dataset contains positive,\\nnegative, uncertain and unmentioned labels. During evaluation we considered the “unmentioned”\\nlabel as negative and included only chest X-rays depicting frontal views.\\nVQA-Rad The VQA-Rad dataset (Lau et al., 2018) comprises 315 radiology images sourced from CT,\\nMRI, and X-ray scans, and it encompasses three anatomical regions including the head, abdomen,\\nand chest. This dataset includes a wide array of question types, spanning 11 distinct categories, such\\nas modality, plane, organ system, abnormality, and more, where 58% of the question-answer pairs are\\ndesigned to be closed-ended (yes/no or limited choices), while the remaining 42% are open-ended.\\n7', metadata={'source': 'data\\\\med.pdf', 'page': 6}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable 1|Overview of the training datasets. More than 7 million data samples from 3.7 million medical\\nimages and cases is used for fine-tuning and further instruction-tuning of Gemini for medical applications\\nin Med-Gemini. This includes diverse set of modalities including 2D and 3D radiology images, pathology,\\nophthalmology, and genomic data. These datasets includes mostly free text paired with medical data, which\\neliminates the need for expensive expert labeling of the training data.\\nModality Dataset No. examples No. Images Description\\nRadiology (2D)Slake-VQA 4,919 450 Radiology images & QA pairs\\nMIMIC-CXR 2,142,892 231,483 Radiology images & free-form reports\\nDigital Knee X-ray 1,469 1,469 Knee X-ray images & labels\\nCXR-US2 132,680 132,680 Radiology images & free-form reports\\nNLST 2,199 2,199 2D CT slices & free-form reports\\nCT-US1 3,207 3,207 2D CT slices & free-form reports\\nRadiology (3D) CT-US1 657,719 657,719 3D CT images & free-form reports\\nPathologyPathVQA 19,654 2,599 Pathology images & QA pairs\\nHistopathology 1,550,976 207,603 Histopathology images, captions, & QA pairs\\nDermatology PAD-UFES-20 2,047 2,047 Skin lesion images & labels\\nOphthalmology EyePACS 14,406 14,406 Fundus images & labels\\nMedical VQAPMC 2,246,656 2,246,656 PubMed Central images & caption pairs\\nMedVQA 12,664 3,168 Medical images & QA pairs\\nGenomics UK Biobank 259,225 259,225 Genomic data & disease outcomes\\nThe standard and official splits of the dataset feature 1,797 QA pairs for training and 451 for\\ntesting purposes. However, due to contamination of images included in both training/test in the\\noriginal dataset release, we constructed a new, non-overlapping test and tuning split for the subset\\nof chest X-ray images and associated question-answer pairs, first described in Xu et al. (2023). In\\nthis study, we went one step further and created new image-disjoint splits of train, validation and\\ntest sets for all three image types. We ensured that the previous X-ray-only validation and test sets\\nwere subsets of the new validation and test sets, respectively, thus enabling comparisons with the\\nELIXR model (Xu et al., 2023) on the new test set, as no former validation examples were included\\nin the new test set. Aside from these constraints, we sampled in a manner that roughly equalizes\\nboth the ratio of closed to open question-answer pairs for each anatomical region, see Table A.5, as\\nwell as the distribution across the 11 different question types across the three splits, see Table A.6 in\\nSection A.1.3. Henceforth, we refer to this new three-way split as the “balanced split”. In total, the\\nbalanced VQA-RAD dataset split, which we will make publicly available soon, comprises 2,248 pairs of\\nquestions and answers, encompassing 1,299 closed-ended questions and 949 open-ended questions.\\nChestX-ray14 The ChestX-ray14 dataset (Summers, 2019) is a comprehensive medical imaging\\ndataset containing 112,120 frontal-view chest X-ray images from 30,805 unique patients. ChestX-\\nray14 builds upon the ChestX-ray8 dataset (Wang et al., 2017), expanding the number of labeled\\ndiseases to fourteen common thoracic pathologies, including Atelectasis, Consolidation, Infiltration,\\nPneumothorax, Edema, Emphysema, Fibrosis, Effusion, Pneumonia, Pleural thickening, Cardiomegaly,\\nNodule, Mass, and Hernia. Because these labels are automatically derived using NLP techniques\\nand therefore contain inherent uncertainty, we restricted our evaluation to a subset of 1,962 cases\\nfocusing on three radiologist-adjudicated conditions (Majkowska et al., 2020), namely lung opacity,\\npneumothorax, and fracture.\\nTCGAstudytype This dataset utilizes histopathology images from The Cancer Genome Atlas (TCGA),\\nfor which different study types correspond to different cancer types with additional information via\\nportal.gdc.cancer.gov. The patches from this dataset are sampled from 2,952 training slides, 1,466\\nvalidation slides, and 1,489 test slides across ten (10) distinct TCGA study types: BLCA, BRCA, COAD,\\n8', metadata={'source': 'data\\\\med.pdf', 'page': 7}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nHNSC, KIRC, LIHC, LUAD, LUSC, OV, and STAD (Lai et al., 2023). We used the test set as an out-of-\\ndistribution dataset to evaluate our model’s ability to generalize to different histopathology-related\\ntasks.\\n2.2.2. Private datasets\\nIND1This is a private research dataset of a similar scale as MIMIC-CXR, which we refer to as\\nIND1 (Nabulsi et al., 2021). This dataset comprises 263,021 de-identified frontal chest X-rays (digital\\nand scanned) along with their corresponding reports. The X-rays were collected from five regional\\ncenters (Bangalore, Bhubaneswar, Chennai, Hyderabad, and New Delhi) across a large hospital group\\nin India between November 2010 and January 2018 (Ahn et al., 2022). We used the same test\\nset as (Tanno et al., 2024), and following their framework, 300 of those cases are used for human\\nevaluation.\\nTTH tissue type The TTH tissue type dataset, introduced by Weng et al. (2019) and Lai et al. (2023),\\nrepresents a patch-level tissue type classification task. This internal dataset comprises 17,319 training\\nslides, 6,488 validation slides, and 6,719 test slides, encompassing a total of 16 distinct tissue types.\\nThese tissue types include Appendix, Breast, Cervix, Colon and Rectum, Fallopian Tube, Gallbladder,\\nLiver, Lymph Node, Ovary, Placenta, Prostate, Skin, Thyroid, Upper GI, Uterus, and Vas Deferens. We\\nused the test set to evaluate generalization of our model to different histopathology tasks.\\n2.3. Data preprocessing\\nRadiology2Dimages When available, images acquired in DICOM format were used to directly create\\nexamples for training and inference. In the case of X-rays, raw pixel data were extracted from the\\nDICOM image pixel data, and the look up table (LUT, part of the DICOM metadata) was subsequently\\napplied. If a DICOM file contained multiple LUTs, we used the first LUT entry. If the window width\\nand window center were defined, these were also used for preprocessing. The final pixel data were\\nre-scaled to the full range of [0,65535]for the 16-bit PNG format. X-ray images in a preprocessed\\nformat were taken as is.\\nCT volumes All 3D CT volumes were derived from DICOM images. Only axial slices were used\\nto establish a standardized anatomical perspective. Slices were sorted based on the Image Posi-\\ntion (Patient) attribute and used to compute slice spacing and reconstruct volumes. Subsequently,\\nimages were clipped with a Hounsfield Unit (HU) range of [−1024𝐻𝑈,1024𝐻𝑈]to cover a full\\nspectrum of densities (e.g. the typical window/level values of brain, soft tissues) and then scaled\\nto[0.0,1.0]. Finally, tricubic interpolation was used to resample all images to a voxel spacing of\\n0.7mm×0.7mm×1.4mm, ensuring consistent uniform resolution for accurate comparative analysis.\\nGenomics: A genomic featurization for an individual consists of polygenic risk scores (PRSs) for 7,415\\ntraits. Each PRS estimates the genetic risk of the individual for a particular disease or trait, calculated\\nby aggregating the estimated effects of many common variants associated with the condition. Each\\nPRS was computed using genome-wide association study summary statistics computed by the Pan-\\nUKB Consortium (Pan-UKB team, 2020). These genomic features were then converted to images by\\nprojecting the PRSs into patch-aligned squares of 8 ×8 pixels with values between [0,255]. The\\n3 RGB channels of the images were used to stack 3 different p-value thresholds of the projections.\\nThe PRS features were obtained from the genetic information of 314,540 individuals of European\\ngenetically inferred ancestry from the UK Biobank (Bycroft et al., 2018; Sudlow et al., 2015).\\nTo create training and evaluation labels, we selected eight in-distribution health outcomes which\\nhave strong heritability (i.e. genetic information plays an important role in influencing susceptibil-\\nity (Visscher etal., 2008)), spanmultipleorgan systems, and arechallengingto predict frompolygenic\\n9', metadata={'source': 'data\\\\med.pdf', 'page': 8}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable 2|Overview of the datasets used for evaluating our fine-tuned Gemini models, Med-Gemini. Our\\nevaluation leveraged a robust dataset suite encompassing 22 datasets across 5 different types of clinically\\nrelevant tasks. This included 8 out-of-distribution datasets to assess generalization and spanned 7 distinct\\nmedical image modalities. We explicitly explored medical image classification, VQA, 2D report generation,\\n3D report generation, and disease prediction from genetic risk embeddings. The total number of evaluation\\nsamples across these datasets exceeds 40,000.\\nTask Dataset/Setup Modality No. Samples OOD\\nRadiology report generation (2D)MIMIC-CXR Chest X-ray 912 -\\nMIMIC-CXR (Expert Evaluation) Chest X-ray 206 -\\nIND1 (Expert Evaluation) Chest X-ray 300 ✓\\nRadiology report generation (3D) CT-US1 (Expert Evaluation) CT 92 -\\nVQAMIMIC-CXR VQA Radiology 226 -\\nSlake-VQA (English-only) Radiology 1,061 -\\nVQA-Rad Radiology 2,248 ✓\\nPathVQA Pathology 6,719 -\\nClassificationPAD-UFES-20 Dermatology 251 -\\nMIMIC-CXR (Abnormal/Normal) Chest X-ray 2,242 -\\nMIMIC-CXR (5 classes) Chest X-ray 2,242 -\\nCheXpert (Abnormal/Normal) Chest X-ray 1,962 ✓\\nCheXpert (5 classes) Chest X-ray 518 ✓\\nChestX-ray14 (3 classes) Chest X-ray 1,962 ✓\\nCAMELYON16 (2 classes) Histopathology 258 -\\nGleason NCB (4 classes) Histopathology 88 -\\nGleason RP (4 classes) Histopathology 202 -\\nLung AD (9 classes) Histopathology 202 -\\nBreast IC (3 classes) Histopathology 669 -\\nBreast NP (3 classes) Histopathology 945 -\\nBreast TF (3 classes) Histopathology 945 -\\nCIN (3 classes) Histopathology 229 -\\nCRC (2 classes) Histopathology 44 -\\nTCGA Study Type (10 classes) Histopathology 1,489 ✓\\nTissue Type (16 classes) Histopathology 6,719 ✓\\nEyePACS Hard Exudates ( 2 classes) Ophthalmology 498 -\\nEyePACS Hemorrhage (2 classes) Ophthalmology 498 -\\nEyePACS DR Lesion(Abnormal/Normal) Ophthalmology 490 -\\nRisk Prediction (UK Biobank)Coronary artery disease Genomic 400 -\\nStroke Genomic 400 -\\nType 2 diabetes Genomic 400 -\\nGlaucoma Genomic 400 -\\nChronic obstructive pulmonary disease Genomic 400 -\\nRheumatoid arthritis Genomic 400 -\\nMajor depression Genomic 400 -\\nAll-cause mortality Genomic 400 -\\nHypertension Genomic 400 ✓\\nHypercholesterolemia Genomic 400 ✓\\nAtrial fibrillation Genomic 400 ✓\\nDiabetic retinopathy Genomic 400 ✓\\nPneumonia Genomic 400 ✓\\nAsthma Genomic 400 ✓\\n10', metadata={'source': 'data\\\\med.pdf', 'page': 9}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nrisk scores alone: coronary artery disease, stroke, type 2 diabetes, glaucoma, chronic obstructive\\npulmonary disease (COPD), rheumatoid arthritis, major depression, and all-cause mortality. Addi-\\ntionally, to assess model generalization, we selected six out-of-distribution (OOD) health outcomes\\nthat share genetic correlation with one or more of the in-distribution health outcomes: hypertension,\\nhypercholesterolemia, atrial fibrillation, diabetic retinopathy, asthma, and pneumonia (Table A.10).\\nPathologypatches Patcheswithinitialsizeof256 ×256pixelsweresampledfromwholeslideimages\\nusing multi-class annotation masks in a class-balanced manner across three different magnification\\nlevels (2, 1, and 0.5 microns-per-pixel).\\nInput preprocessing and tokenization Images from all 2D datasets were uniformly resized to\\n768×768 pixels, preserving aspect ratio with padding, with pixel intensities scaled to [0,1]. This\\nensured image resolution would be high enough for the fine-grained detail of medical images. For\\ntext, we used the native Gemini SentencePiece tokenizer (Gemini Team, Google, 2023; Kudo and\\nRichardson, 2018) without modification.\\n3. Modeling Methodology\\n3.1. Model architecture\\nGemini builds upon the robust foundation of Transformer decoders (Parmar et al., 2018; Vaswani\\net al., 2017), offering significant architectural and optimization enhancements for efficient, stable\\nlarge-scale training (Barham et al., 2022; Gemini Team, Google, 2023). This equips Gemini with\\nexceptional natural language understanding and text generation capabilities. Of particular interest\\nfor medical data processing, Gemini’s multimodal design draws inspiration from foundational Google\\nresearch on Flamingo (Alayrac et al., 2022), CoCa (Yan et al., 2022; Yu et al., 2022) and PaLI (Chen\\net al., 2022), enabling enhanced multimodal understanding and reasoning.\\nGemini handles video understanding by encoding frames as a sequence within its large context\\nwindow (Gemini Team, Google, 2023; Google, 2024). This allows seamless integration of video\\nframes, multi-slice images, text, or audio inputs. The model even supports variable input resolutions,\\nenabling it to prioritize computational resources for tasks requiring high-resolution analysis. Gemini\\n1.5 specifically is a mid-size model with a context window of up to 1 million tokens and performance\\non par with the largest Gemini model, 1.0 Ultra. Given this exceptional efficiency, we chose to finetune\\nMed-Gemini from Gemini 1.5.\\n3.2. Multimodal fine-tuning\\nThree custom versions of the Gemini 1.5 Pro vision encoder were trained for 2D modalities, 3D\\nmodalities, and genomics. In our initial experiments, we found that custom vision encoders for each\\ntype of data format performed better than a single vision encoder for all data formats. Furthermore,\\nfine-tuning the vision encoder as well as the language component in Gemini led to significantly better\\nvisual understanding in comparison to a model that used the native vision encoder of Gemini 1.5\\nPro models. From these three custom vision encoders, we trained three specific variants of Med-\\nGemini which we refer to as Med-Gemini-2D, Med-Gemini-3D, and Med-Gemini-Polygenic. Notably,\\nMed-Gemini-2D includes all conventional medical images that are encoded in 2D (e.g. chest X-ray,\\nCT slices, pathology patches), Med-Gemini-3D was built on top of Med-Gemini-2D and handles 3D\\nmedical data (e.g. CT), and Med-Gemini-Polygenic was trained for a novel image encoding derived\\nfrom non-image features (e.g. genomics). For all three model variants, fine-tuning was framed as a\\ncaptioning or VQA task.\\n11', metadata={'source': 'data\\\\med.pdf', 'page': 10}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nFine-tuning for 2D modalities - Med-Gemini-2D All 2D modalities were fine-tuned together using\\nthe training mix described in Section 2 and Table 1 to create Med-Gemini-2D. The 2D modalities used\\nfor fine-tuning included the described radiology, pathology, dermatology, and ophthalmology images.\\nFine-tuning for 3D modalities - Med-Gemini-3D To interpret 3D medical data, we leveraged the\\nvideo understanding capabilities of Gemini (Google, 2024). Use of the Gemini video encoder allows\\nMed-Gemini-3D to process multiple 2D slices, replacing the time axis with the depth dimension, with\\ncomputed tomography (CT) as our example modality. This 3D fine-tuned model can then synthesize\\ninformation across a series of 2D slices to generate radiology reports. Use of this video encoding\\ncapability will permit analysis of other volumetric and time-series medical data (e.g. MRI, ultrasound)\\nin the future.\\nFine-tuning for genomics - Med-Gemini-Polygenic Genomics “images” (polygenic risk scores (PRS)\\nprojected into 2D, see Section 2.3) were included in the mixture of datasets used to fine-tune the\\nMed-Gemini-Polygenic vision encoder, and were trained to predict eight broad health outcomes\\n(coronary artery disease, stroke, type 2 diabetes, glaucoma, chronic obstructive pulmonary disease,\\nrheumatoid arthritis, major depression, and all-cause mortality) in a captioning task.\\nInstruction fine-tuning To optimize the instruction-following capabilities of the fine-tuned Med-\\nGemini even further, we subsequently employed an instruction-tuning phase. In this phase, we\\nfine-tuned Gemini 1.5 Pro on a curated collection of multimodal data consisting of carefully crafted\\ninstruction and response pairs. By exposing the model to these examples, we refined its ability to not\\nonly understand the content of medical images and signals, but also to follow nuanced instructions\\nand generate tailored outputs.\\n3.3. Model training and inference infrastructure\\nLike its predecessor Gemini 1.5 Pro and all other Gemini models, Med-Gemini was trained on\\nlarge-scale Google TPUv4 accelerator pods spread across multiple data-centers. This training setup\\nsignificantly scales up from our previous flagship PaLM family (Chowdhery et al., 2023). The Gemini\\narchitectureensuresefficientservingonTPUacceleratorsatscale. Fordetailedinformationontraining\\nand serving Gemini models, see (Gemini Team, Google, 2023; Google, 2024).\\n4. Evaluation and Results\\nThefollowingsectionsexploreindetailhowGeminiandMed-Geminiperformacrossvariousmodalities\\nand tasks in the medical field. Due to restrictions in our data licenses, our evaluation was limited to\\ninternal models. Our evaluation leveraged a robust dataset suite encompassing 22 datasets across\\nfour different clinically relevant tasks (report generation, VQA, classification, risk prediction). This\\ndataset includes eight out-of-distribution datasets to assess generalization and spanned seven distinct\\nmedical image modalities. An overview of the evaluation datasets is provided in Table 2. The total\\nnumber of evaluation samples across these datasets exceeded 40,000.\\n4.1. Medical image classification\\nTorigorouslyevaluateMed-Gemini’sin-distributionandout-of-distributionperformance, weemployed\\na comprehensive medical image classification benchmark. This benchmark encompassed diverse\\nmodalities: skin lesion classification, chest X-ray classification, histopathology patch classification,\\nand fundus image classification. We approached classification as a generative multi-choice task for\\nzero-shot classification (no supporting example in prompt) and linear probing for label-efficient\\nsetups. This design allowed for a thorough assessment of Med-Gemini’s robustness and adaptability\\n12', metadata={'source': 'data\\\\med.pdf', 'page': 11}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nacross various medical imaging domains.\\nChest X-ray image classification Our chest X-ray image classification evaluation focused on two\\nkey classification scenarios. First, we considered multi-label classification for the presence of each\\nof five types of frequently occurring conditions: atelectasis, cardiomegaly, consolidation, pulmonary\\nedema, and pleural effusion. This follows the suggestions from Azizi et al. (2021, 2023); Irvin et al.\\n(2019); Tanno et al. (2024). Second, we performed binary classification for all images as either\\nnormal or abnormal, based on the CheXpert “no finding” label for frontal chest X-rays (Irvin et al.,\\n2019). These two scenarios are used consistently across MIMIC-CXR and our out-of-distribution\\ndataset CheXpert (Irvin et al., 2019). In addition, for ChestX-ray14 (Summers, 2019; Wang et al.,\\n2017) we focused the evaluation of our model on three specific conditions including lung opacity,\\npneumothorax, and fracture. For all test tests, we only included images that were frontal view (i.e.\\nview position “AP” or “PA”). In addition, for MIMIC-CXR it is required the original report to contain a\\n“Findings” section that could be extracted via regular expression matching.\\nWe manually explored the validation set, prompting each model either with a multi-select prompt\\nfor all labels ( i.e.,5 conditions plus normal/abnormal) at once, or multiple binary Yes/No prompts for\\neachlabelseparately, andfoundbinarypromptstoyieldbetterMacroF1resultsforMed-Geminiforall\\nlabels,andslightlybetterresultsforGeminiUltraexceptforpredictingnormal/abnormal. Theprompts\\nused for evaluation are listed in Section A.1.2. Answers were generated using nucleus sampling with\\na temperature of 0.0, a top_p of 0.75 and an output token limit of 200. Generated answers were\\nnormalized and matched against “yes”/“no” ground truth strings, which directly corresponded to 1.0\\nand 0.0 label values for all evaluation data sets. For multi-label, multi-class scenarios, we evaluated\\nthe average accuracy using the class-weighted F1 score. Details of the metrics used can be found\\nin Section A.3. The MIMIC-CXR labels were revised based on a selective review of flagged reports\\nby board-certified radiologists. See Section A.1.1 for more details about the revised MIMIC-CXR\\nlabels. For the MIMIC CXR evaluations, we excluded case/condition combinations with an “uncertain”\\n(-1.0) or no label (blank), except for the “No Findings” condition, where all cases with a non-positive\\n(1.0) label were considered negative. Classification results using data-efficient learning are described\\nseparately in Section A.2.1.\\nTable 3 shows the comparison of the performance on the chest X-ray classification task between\\nMed-Gemini and Gemini Ultra for in- and out-of-distribution datasets. Our medically tuned model\\noutperformed Gemini Ultra across most labels on the in-distribution MIMIC-CXR dataset. Notably, we\\ndemonstrated significantly stronger performance on the normal/abnormal classification despite using\\na multi-select prompt for Gemini, which specified all the “abnormal” conditions (and yielded better\\nresults on the validation set than a dedicated binary prompt), versus a very short normal/abnormal\\nbinary prompt for Med-Gemini. However, on the more challenging out-of-distribution datasets\\n(CheXpert and ChestX-ray14), performance is varied. Med-Gemini excels in some tasks such as\\ncardiomegaly or pleural effusion detection on CheXpert, while lagging in others like fracture detection\\nin ChestX-ray14, which is a strong minority class there. These results suggest room for improvement\\nin handling significant domain shifts.\\nHistopathology image classification We evaluated the image embeddings of Med-Gemini-2D via\\nlinear probing on the 11 tasks from Lai et al. (2023) and summarized in Table A.8. Med-Gemini was\\nfine-tuned on data corresponding to 9 of these tasks (in-distribution), while 2 tasks were held-out\\n(out-of-distribution). Together, these evaluation tasks cover a total of 17 tissue types, 12 cancer\\ntypes, and several different types of classification tasks ( e.g.,tumor identification, grading, subtyping)\\nacross 3 magnifications. Linear probing was done as in Lai et al. (2023): briefly, a logistic regression\\nmodel with L2-regularization was fit for each task, and task-specific regularization weights and\\nmagnifications were selected using the validation sets. Linear probe metrics on the test sets were\\ncalculated using 5,000 patches with logistic regression models trained on embeddings from the\\n10,000 train set plus 5,000 validation set patches. Confidence intervals for macro-averaged AUCs were\\n13', metadata={'source': 'data\\\\med.pdf', 'page': 12}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable3|Med-Gemini-2DPerformanceonthechestX-rayclassificationtask. Comparisonoftheperformance\\non the chest X-ray classification task between Med-Gemini, Gemini Ultra and also SoTA, if comparable or\\navailable. For MIMIC-CXR datasets we utilized the revised labels (see Section A.1.1) denoted by †. Our\\nmedically fine-tuned model demonstrated superior performance on the in-distribution MIMIC-CXR dataset.\\nHowever, for out-of-distribution datasets, Med-Gemini excelled in specific tasks such as Cardiomegaly detection,\\nbut both Gemini Ultra and Med-Gemini fell short of models exclusively trained for chest X-ray classification,\\nand for strong minority class with distinctly different visual features such as Fracture.\\nDataset ConditionF1 score(%)OODMed-Gemini Gemini Ultra SoTA\\nCheXpertAtelectasis 49.7 51.1 64.6‡\\n✓Cardiomegaly 72.0 51.5 74.3‡\\nConsolidation 23.0 17.9 33.3‡\\nEdema 32.7 46.2 60.2‡\\nPleural Effusion 64.4 46.6 70.4‡\\nMacro-F1 48.3 42.6 60.6‡\\nNormal/Abnormal 54.3 52.7 -\\nChestX-ray14Lung Opacity 79.9 73.3 88.8∗\\n✓Pneumothorax 55.3 29.3 58.4∗\\nFracture 5.5 0 27.8∗\\nMacro-F1 46.7 34.2 58.3∗\\nMIMIC-CXRAtelectasis† 99.8 88.1\\n§-Cardiomegaly† 94.1 94.6\\nConsolidation† 82.0 77.0\\nEdema† 86.8 86.4\\nPleural Effusion† 90.8 88.1\\nMacro-F1† 90.7 86.8\\nNormal/Abnormal 42.0 29.7\\n†Revised labels (Park et al., 2024).\\n‡Results from CheXzero model (Tiu et al., 2022).\\n§SoTA not compatible with test set nor labels.\\n∗Labels and prediction from Majkowska et al. (2020). F1 scores computed from their PPV and sensitivity.\\ncomputed via blocked bootstrap (blocking on slides) with 10,000 replicates. For comparison, we also\\nreport performance with embeddings from an ImageNet21k-based ViT-S/16 model trained using the\\nAugReg method (Steiner et al., 2021), embeddings produced by the vision encoder in Gemini Ultra,\\nand embeddings from a histopathology-specialized model trained via self-supervision (Lai et al., 2023)\\n(PathSSL). Results are reported in Figure 2. While the PathSSL embedding model is specialized to the\\nhistopathology domain, the image embeddings in Med-Gemini-2D achieved comparable performance\\nwhile also demonstrating strong results across multiple other clinical domains.\\nSkin lesion classification Med-Gemini-2D achieves competitive classification accuracy using just\\ndermatological images alone as input, and does not rely on metadata (e.g. patient demographics,\\nlesion symptoms, living conditions). Such metadata, while provided in PAD-UFES-20, are not always\\nreadily available in clinical settings. We note that our evaluation is not directly comparable to Med-\\nPaLM M (Tu et al., 2024) since (a) Med-PaLM M inputs an additional 14 clinical attributes and (b)\\nwe created different train and test splits to remove patient overlap between splits in the original\\nMed-PaLM M work (we hope to publicly release these updated splits soon). To establish context for\\nmodel performance, we compared Gemini performance with Derm Foundation (Google, 2024) which\\nis a specialized dermatology model developed by Google. We trained linear probing classifiers on top\\nof the Derm Foundation embeddings in this comparison.\\n14', metadata={'source': 'data\\\\med.pdf', 'page': 13}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nBreast NP Breast TFCIN\\nGleason RPGleason NCBBreast IC Lung ADCRC\\nCAMELYON1665707580859095100AUC (%)AugReg ImageNet \\nGemini Ultra\\nMed-Gemini\\nPathSSL\\n(a)In-distribution\\nTissue type TCGA study (b)Out-of-distribution\\nFigure 2|Med-Gemini-2D histopathology image classification performance. Linear-probing on histopathol-\\nogy patch-classification tasks (macro-averaged AUC percentages with 95% confidence intervals) on our in-\\ndistribution and held-out out-of-distribution tasks. Overall, while Med-Gemini-2D outperforms Gemini Ultra on\\n7 out of 9 in-distribution tasks and on both out-of-distribution tasks, it does not improve over a histopathology-\\nspecific foundation model (PathSSL) on any of our tasks.\\nTable 4|Performance on PAD-UFES-20 classification task. AUC linear probing demonstrates that Gemini\\nUltra and Med-Gemini-2D generate robust skin lesion classification embeddings, with Med-Gemini-2D further\\napproaching the performance of the specialized Derm Foundation model.\\nMetric Weighted-AUC (%) Weighted-F1 (%) Accuracy (%)\\nGemini Ultra92.6\\n(89.8, 95.1)60.3\\n(54.6, 66.9)60.6\\n(54.1, 66.8)\\nMed-Gemini-2D92.1\\n(89.2, 94.7)71.4\\n(65.4, 77.4)73.3\\n(67.7, 78.9)\\nDerm Foundation (Google, 2024)94.0\\n(91.6, 96.1)77.0\\n(71.4, 82.4)76.9\\n(71.3, 82.5)\\nWe utilized the following three metrics for evaluation. (1) Weighted-AUC (by class prevalence):\\nWe first extracted the embedding outputs from Med-Gemini-2D’s image encoder, Gemini Ultra’s image\\nencoder and Derm Foundation. Then we individually trained linear probes on top of the embeddings\\non the entire PAD-UFES-20 train split to classify 6 skin lesion types, and computes their weighted-AUC\\non the test split. (2) Weighted-F1 (by class prevalence): For Med-Gemini-2D and Gemini Ultra,\\nwe extracted classification prediction based on string matching from the model output. For Derm\\nFoundation, we took the argmax of the linear probe prediction as the predicted class. (3) Accuracy:\\nWe computed the classification prediction follows the same method as in weighted-F1.\\nTable 4 shows the performance comparison. From the AUC linear probing results, we can see that\\nboth Gemini Ultra and Med-Gemini-2D produced robust embeddings for skin lesion classification and\\nhad on-par performance when compared with the specialized Derm Foundation model. From F1 and\\naccuracy, we can see that the fine-tuning in Med-Gemini-2D improved the LLM’s understanding of\\nthe embedding space for skin lesions and achieved performance close to the Derm Foundation model.\\nFundus image classification We evaluated the performance of Med-Gemini-2D on four ophthalmol-\\nogy classification tasks. First, we approached the identification of three common Diabetic Retinopathy\\n(DR) lesions, more specifically hard exudates, hemorrhage, and panretinal photocoagulation (PRP)\\nscars, as a multi-label classification challenge. Then, we treated anomaly detection as a binary classifi-\\ncation problem, where the model was tasked with determining the presence or absence of DR lesions\\nin a fundus image. EyePACS dataset (Cuadros and Bresnick, 2009) was used for all tasks, with fundus\\nimages balanced for equal distribution of positive and negative labels.\\n15', metadata={'source': 'data\\\\med.pdf', 'page': 14}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable 5|Performance comparison of Med-Gemini-2D, Gemini Ultra, and a supervised model that is\\ntrained with additional data for fundus image classification. For all tasks, Med-Gemini-2D demonstrates\\nsignificant improvement over Gemini Ultra. For lesion presence classification, Med-Gemini-2D is on-par with\\nthe supervised model for sensitivity, while specificity still have room for improvement (likely due to orders of\\nmagnitude less data used during the training process).\\nCondition ModelAccuracy\\n(%)Sensitivity\\n(%)Specificity\\n(%)F1 score\\n(%)\\nHard exudatesGemini Ultra 55.2 71.5 39.0 61.5\\nMed-Gemini-2D 88.4 80.3 96.4 87.3\\nHemorrhageGemini Ultra 49.0 69.9 28.1 57.8\\nMed-Gemini-2D 84.7 88.4 81.1 85.3\\nPRP ScarsGemini Ultra 57.2 56.8 57.6 57.0\\nMed-Gemini-2D 84.6 71.6 97.6 82.3\\nDR lesions presentGemini Ultra 58.6 73.6 43.8 63.9\\nMed-Gemini-2D 84.9 96.3 73.5 86.4\\nSupervised model trained\\nwith additional data94.7 96.8 92.6 94.9\\nWe benchmarked Med-Gemini-2D against Gemini Ultra, which used both the fundus image and a\\nmultiple-choice-like prompt for prediction. To extract the prediction labels, we searched for specific\\nmarkers that the LLM was constrained to output (e.g. (G) if no DR lesion is present). In contrast,\\nMed-Gemini-2D relied only on the image, with prediction labels extracted by searching for keywords,\\nlike “hemorrhage.” For the anomaly detection task, we compare to a third model similar to (Krause\\net al., 2018) that has been trained using supervised learning to detect different grades of DR (none,\\nmild, moderate, severe, proliferative). If the predicted DR grade is “none,” the fundus is considered\\nnormal (no DR lesion), otherwise, the image is predicted to have DR lesions present. We note that\\nthis supervised model was carefully trained on a much larger dataset containing more than 3 million\\nfundus images from diverse manufactures/data sources/geography, and could be considered as an\\n“upper bound” of this task.\\nTable 5 displays the performance of Med-Gemini-2D and Gemini Ultra on the classification of\\nhard exudates, hemorrhages and PRP Scars, utilizing accuracy, sensitivity, specificity, and F1 score\\nas evaluation metrics. It also compares the performance of DR lesions detection between, Med-\\nGemini-2D, Gemini Ultra and the strong supervised model. Results demonstrate that Med-Gemini-2D\\nconsistently outperformed Gemini Ultra on both multi-label and binary classification tasks. Notably,\\nMed-Gemini-2D achieved significantly higher specificity in hard exudate classification (96.4% vs.\\nGemini Ultra ’s 39.0%) and hemorrhage classification (81.1% vs. Gemini Ultra ’s 28.1%). These\\nresults highlight the benefits of task-specific fine-tuning for this highly-specialized medical domain.\\nMed-Gemini-2D underperformed on the anomaly detection task compared to the strong supervised\\nmodel, but it is important to acknowledge the strong supervised model’s significant advantage in the\\n∼200×volume of labeled data used during its training process.\\nFor three other attempted classification tasks, namely detection of microaneurysms, neovas-\\ncularization of the optic disc and neovascularization elsewhere, Med-Gemini-2D appeared to be\\nmiscalibrated, predicting most cases as negative in the LLM text output. While this is likely related to\\nthe training dataset distribution and overall data mixing ratio, further work is needed to improve\\nquestion-answering based classification and calibration.\\n16', metadata={'source': 'data\\\\med.pdf', 'page': 15}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable 6|Evaluation details for VQA tasks. Our Med-Gemini-2D’s VQA performance was evaluated across\\ndiverse medical specialties using various datasets. Notably, in radiology tasks, Med-Gemini-2D outperformed\\nGemini and previous best-in-class performance across different subsets and metrics, improving best-in-class\\nperformance by over 11%. Performance is reported using Mean Tokenized F1-score, Mean Expert Score,\\nand Accuracy. Based on radiologist assessments, we excluded several cases from the VQA-Rad dataset due to\\nquestions being deemed unanswerable from the provided images. For VQA-Rad we report the accuracy for\\nthe balanced test split and the test split suggested by Xu et al. (2023). In pathology, it showed reasonable\\nperformance on this useful, albeit noisy dataset, as measured by accuracy for Yes/No questions and average\\ntokenized F1-score for zero-shot responses.\\nDataset Subset Metric(%) Med-Gemini Gemini Ultra SoTA\\nVQA-RadTest set from\\nXu et al. (2023)\\n(CXR only)Expert score\\n(Excluding 12 cases)71.9 -57.9\\n(Xu et al., 2023)\\nAccuracy\\n(Closed only)78.8 -67.1\\n(Xu et al., 2023)\\nBalanced Test\\n(CXR only)Expert score\\n(Excluding 4 cases)71.8 -55.7‡\\n(Xu et al., 2023)\\nTokenized F1 62.6 49.0 -\\nAccuracy\\n(Closed only)78.1 62.4 -\\nBalanced Test\\n(CXR, CT, MR)Expert score\\n(Excluding 44 cases)61.9 - -\\nTokenized F1 50.1 46.4 -\\nAccuracy\\n(Closed only)69.7 64.9 -\\nSlake-VQAOfficial Test\\n(English only)Tokenized F1 75.8 44.689.3†\\n(Tu et al., 2024)\\nAccuracy\\n(Closed only)84.8 70.491.1\\n(Li et al., 2023b)\\nPathVQA Official TestTokenized F1 58.7 34.962.7†\\n(Tu et al., 2024)\\nAccuracy\\n(Closed only)83.3 62.890.9\\n(Sun et al., 2024)\\nMIMIC-CXR\\nVQATest split from\\nXu et al. (2023)Tokenized F1 52.5 44.8 -\\nAccuracy\\n(Closed only)78.6 70.968.1\\n(Xu et al., 2023)\\n†Using one-shot prompt with a text-only exemplar (Tu et al., 2024).\\n‡Average of original scores over new, smaller test set.\\n4.2. Visual question answering (VQA)\\nWe assessed Med-Gemini-2D’s performance on VQA tasks across a range of diverse medical specialties,\\nincluding radiology, dermatology, and pathology, and spanning a wide range of open-ended and\\nclosed-ended questions. Table 6 summarizes the overall VQA results. Prompt templates were manually\\noptimized for each model and VQA dataset on the validation splits, and are listed in Table A.2.\\nModel answers were generated using the same method and parameters as for CXR classification, see\\nSection 4.1. That is, the generative sampling was not constrained by a given test vocabulary in any\\nmanner, as it was in related work (Li et al., 2023a,b; Zhang et al., 2023a), typically only to the test\\nset’s ground truth answers, for the reasons described e.g. in Tu et al. (2024); Van Sonsbeek et al.\\n(2023). In other words, answers were generated in a truly generative, open-ended, and zero-shot\\nmanner.\\n17', metadata={'source': 'data\\\\med.pdf', 'page': 16}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nFor close-ended questions, we measured accuracy based on exact matches of normalized model vs\\ngroundtruthanswers,andcompareagainstSOTAbasedonvocabulary-constrainedanswergenerations,\\nsince the freely generated answers mostly matched the overall set of ground truth answers. For open-\\nended questions, we report the average token-wise F1 score (Tu et al., 2024) between the normalized\\nanswers of the model and the ground truth, and only compare against SOTA results where answers\\nwere generated in the same zero-shot manner. In addition, for Med-Gemini-2D results on VQA-Rad,\\none board-certified radiologist scored the answers using the 3-point scoring rubric introduced in Xu\\net al. (2023), in order to compare against results of the prior ELIXR model.\\nWe assessed Med-Gemini-2D’s VQA capabilities in radiology using three datasets from distinct\\ndomains. First, weevaluatedontheMIMIC-CXRVQAtestset, anin-distributionbenchmarkcontaining\\n226 question-answer pairs for 48 chest X-ray images suggested by Xu et al. (2023). Second, we used\\nthe English-only 1,061 question and answer pairs in the test split of Slake VQA, a large bilingual\\n(English and Chinese) VQA dataset. Finally, we employed the VQA-Rad dataset, leveraging the new\\nthree-way balanced split detailed in Section 2. As discussed previously, to evaluate out-of-distribution\\nperformance and facilitate a head-to-head comparison with the previous best-in-class model (ELIXR),\\nwe did not fine-tune our model on either the VQA-Rad training images or questions. This approach\\nensures both ELIXR and our model are tested on the same, larger VQA-Rad test set (Xu et al., 2023).\\nMoreover we evaluated the performance of our model on both chest X-ray only and all modality (CT,\\nMRI, and X-ray).\\nAs Table 6 demonstrates, Med-Gemini-2D outperformed many previous results and Gemini across\\ndifferent subsets and metrics. Specifically, in the chest X-ray only (ELIXR split) subset, our model\\nachieved a remarkable expert-evaluated accuracy score of 71.9 and an accuracy of 78.8 in closed-\\nended questions, improving the best-in-class number by 14 and 11.7, respectively. In the chest\\nX-ray-only balanced split subset, our model maintained strong performance with an expert-evaluated\\naccuracy of 71.8 improving best-in-class number by 16 and an accuracy of 78.1 in closed-ended\\nquestions. Moreover, across all modalities in the balanced split, our model achieved competitive\\nresults and improved over Gemini, demonstrating its versatility. In the MIMIC-CXR VQA dataset, it\\nachieved an accuracy of 78.6% on Yes/No questions and a tokenized F1 score of 52.5 overall. In the\\nSlake VQA dataset, our model significantly outperformed Gemini and achieves performance close\\nto state-of-the-art with 84.8 accuracy on close-ended questions, showcasing its capability across\\ndifferent domains. Its mean tokenized F1-score across all English questions at 75.8 is lower than\\nfor the MedPaLM-M model (Tu et al., 2024) at 89.3, which might be partially attributable to it\\nbeing prompted in a zero-shot manner, versus a one-shot text-only prompt for the latter. Contrary\\nto MedPaLM-M, Med-Gemini-2D was not fine-tuned with one-shot examples, hence this prompting\\ntechnique would yield worse results during inference.\\nTo evaluate Med-Gemini-2D’s VQA capabilities in pathology, we utilized the PathVQA dataset (He\\net al., 2020). For this dataset, our model achieved an accuracy of 83.3 at Yes/No questions, and\\na tokenized F1-score of 58.7 over all questions which improves over Gemini. Its overall zero-shot\\nresults are slightly below those of the MedPaLM-M model (Tu et al., 2024), which reports an overall\\ntokenized F1-score of 62.7, albeit employing a text-only one-shot prompting technique here as\\nwell. This technique involved an additional exemplar question-and-answer pair along with an image\\nplaceholder string <img> provided as a one-shot example during evaluation. However, while these\\nresults can provide a general sense of VQA capabilities for images comprising both histopathology and\\ngeneral anatomic pathology photographs and diagrams, given the known issues with QA pairs and\\nimage quality in this auto-generated dataset (Lu et al., 2024), we suggest cautious interpretation.\\nWe also conducted a qualitative review of model behavior for histopathology and radiology VQA\\ntasks. Examples are shown in Figure 6, and Figure 7 .\\n18', metadata={'source': 'data\\\\med.pdf', 'page': 17}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable 7|Evaluation details report generation in chest X-rays. Med-Gemini-2D sets a new standard for AI\\ngenerated chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results\\nacross two separate datasets.\\nDataset Metric Med-Gemini SoTA\\nMIMIC-CXRAI superior or similar to original report (Normal) 57% 45% (Tanno et al., 2024)\\nAI superior or similar to original report (Abnormal) 43% 42% (Tanno et al., 2024)\\nClinically acceptable AI generated report (All) 72% -\\nIND1AI superior or similar to original report (Normal) 96% 85% (Tanno et al., 2024)\\nAI superior or similar to original report (Abnormal) 65% 53% (Tanno et al., 2024)\\nClinically acceptable AI generated report (All) 88% -\\n4.3. Report generation for chest X-rays\\nIn clinical practice, the role of the radiologist extends far beyond narrow interpretation of radiology\\nimages. Radiologists are tasked with conveying nuanced findings within a broader clinical context,\\nsynthesizing information, and providing recommendations for patient care. Expert radiologists use\\nnatural language to articulate this synthesis of imaging findings, overall impressions, and recom-\\nmendations in written reports. Unlike some prior work, our model was tuned for the difficult task\\nof generating both the ‘FINDINGS’ and ‘IMPRESSION’ sections of chest X-ray reports for frontal\\nview chest radiographs (anterior-posterior or posterior-anterior), covering comprehensively both the\\nobservations and inferences typically made by radiologists during a study.\\nTable 8 presents the performance comparison of various models in generating radiology reports\\nfor chest X-rays using the publicly available MIMIC-CXR dataset. The “Sections” column indicates\\nwhether the model generates the ‘FINDINGS’ (‘F’) or ‘IMPRESSIONS’ (‘I’) section of the report, with\\nmetrics drawn from published research. Higher values in all metrics indicate superior performance.\\nNotably, our model undertakes the more challenging task of generating both sections (F + I) for\\nfrontal chest X-rays, aiming to capture the radiologist’s holistic interpretation of the study.\\nFollowingcommonpractice, weleveragedtheestablishedn-grambasedmethodssuchasROUGE-L,\\nBLEU-4 to evaluate the generated reports quality against the ground-truth. Additionally, we measured\\nthe RadGraph F1-score, which is the F1 score between the entities extracted from the reference report\\nand generated one using RadGraph (Jain et al., 2021). RadGraph accounts for not only the absence\\nor presence of findings in the report, but also their relationships to image features. Med-Gemini\\nachieved a RadGraph F1-score of 24.4%, marking a notable improvement of 3.9% compared to the\\nprevious top-performing model.\\nFor the IND1 dataset we did not evaluate automated metrics, as automated metrics such as\\nRadGraph F1-score are specifically trained on MIMIC-CXR to measure performance of US-style chest\\nX-ray report and are not capable of handling the out-of-distribution format of IND-1 dataset reports\\nobtained in an India-based clinical setting.\\nHumanevaluationrubricforreportgeneration For report generation we devised a novel evaluation\\nrubric, expanding on those used in Flamingo-CXR (Tanno et al., 2024) and Med-PaLM M (Tu et al.,\\n2024), to understand potential impact on clinical management of patients. The evaluation rubric\\nconsists of six categories that compare two reports. It provides an improved granularity around\\npatient impact and was used for both the CXR and CT generated reports. Table 9 defines the labels\\nfor comparing the AI and original radiologist reports for the same study. This rubric along with\\ntraining materials and examples were provided to radiologist labelers as training material, prior to\\nany labeling. In each example seen by labelers, the origin of the reports (AI vs. original) was masked\\nand the reports were shown in random order to avoid bias.\\n19', metadata={'source': 'data\\\\med.pdf', 'page': 18}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable 8|Automated report generation metrics on the MIMIC-CXR dataset. This table presents the\\nperformance of various models on generating radiology reports for chest X-rays using the publicly available\\nMIMIC-CXR dataset. The Sections column indicates whether the model generates the FINDINGS (F) or\\nIMPRESSION (I) section of the report, with metrics sourced from published research. For all of the metrics\\nhigher is better. Bold values highlight the best results in each category for (F + I) methods. Notably, our model\\ntackles the more challenging task of creating both sections (F + I) for frontal chest X-rays (anterior-posterior\\nor posterior-anterior views), aiming to capture the radiologist’s comprehensive interpretation of the study.\\nMed-Gemini achieved a RadGraph F1-score of 24.36% on chest X-ray report generation, demonstrating a 4.0%+\\nimprovement over the previous best-in-class score.\\nModel SectionsNLG Metrics(%) Clinical Metrics(%)\\nCIDEr BLEU4 Rouge-L RadGraph F1-score\\nCXR-RePaiR (Endo et al., 2021) F - 2.1 14.3 9.1\\nM2Transformer (Miura et al., 2020) F 50.9 11.4 - 22.0\\nRGRG (Tanida et al., 2023) F 49.5 12.6 26.4 -\\nMETransformer (Wang et al., 2023a) F 36.2 12.4 29.1 -\\nMed-PaLM M, 12B (Tu et al., 2024) F 23.4 10.4 26.2 25.2\\nMed-PaLM M, 84B (Tu et al., 2024) F 26.2 11.3 27.3 26.7\\nMAIRA-1 (Hyland et al., 2023) F - 14.2 28.9 24.3\\nR2Gen (Chen et al., 2020) F + I - 10.3 27.7 13.4\\nWCT (Yan et al., 2021) F + I - 14.4 0.274 14.3\\nCvT-21DistillGPT2 (Nicolson et al., 2023) F + I 36.1 12.4 28.5 15.4\\nBioVil-T (Bannur et al., 2023) F + I - 9.2 29.6 -\\nR2GenGPT (Wang et al., 2023b) F + I 26.9 13.4 29.7 -\\nFlamingo-CXR (Tanno et al., 2024) F + I 13.8 10.1 29.7 20.5\\nMed-Gemini-2D F + I 17.5 20.5 28.3 24.4\\nFive India-based board-certified radiologists, one India-based thoracic specialist, and one US-based\\nacademic thoracic radiologist evaluated a total of 606 cases: 306 from the MIMIC test dataset and\\n300 from IND1. After the study completion, readers were compared using their mean Quadratic\\nKappa (Sim and Wright, 2005) to the two thoracic specialists. Two readers falling below 0.2, i.e.\\n“none to slight agreement” were eliminated from the final results. The results were computed based\\non the total sum of categories for the selected reports after elimination of scores of the Xcategory.\\nThe percentage of cases within each category were then plotted sequentially along a horizontal plot\\nfor all, abnormal, and normal cases as shown in Figure 3 and summaries are shown in Table 7\\nIn examining cases that fell into the A1andB1categories, i.e. where one report captures clinical\\nfindings but both would result in the same patient management, similar reasoning was given in both\\ncategories. These included missing less critical findings and descriptiveness of findings. Examples\\nof missed findings include: mild cardiomegaly, calcified granulomas, and old fractures. In terms\\nof descriptiveness, examples include: better descriptions of bulla, proper identification of devices,\\nand clearly discerning mass versus pneumonia and other less explicit diagnoses. Reports falling into\\ncategories A2andB2missed key findings, including: failures in assessing tube positions, missed\\nnodules, and missed pneumothraces.\\n4.4. Report generation for head/neck CT volumes\\n3D imaging modalities often involve more complex data preparation and longer radiologist interpreta-\\ntion time in comparison to 2D images such as X-rays, making the paired image-text data required for\\ngenerative AI modeling scarcer and more expensive. Additionally, radiology reports tend to be much\\nlonger and imaging features much sparser for 3D images than for 2D images. Given this relative\\n20', metadata={'source': 'data\\\\med.pdf', 'page': 19}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable 9|Human evaluation rubric comparing AI generated radiology reports to original reports.\\nRubric Score Rubric Definition\\nA2Report A captures key clinically relevant findings that are not found in B. Report A\\nwould result in correct patient management and report B would not.\\nA1Report A captures more relevant findings, but both would result in the same correct\\npatient management.\\nCBoth reports capture similar findings in the image and would result in correct pa-\\ntient management.\\nB1Report B captures more relevant findings, but both would result in the same correct\\npatient management.\\nB2Report B captures key clinically relevant findings that are not found in A. Report B\\nwould result in correct patient management and report A would not.\\nX Neither report would result in correct patient management.\\ndata scarcity and information complexity (with correspondingly increased memory requirements),\\nend-to-end modeling to convert 3D radiology images to text reports has previously been infeasible.\\nWith its increased computational capacity and extensive domain-specific pretraining, Med-Gemini-3D,\\nbuilding on other recent generative AI work such as Hamamci et al. (2024), is the first LLM-based\\ngenerative AI model able to interpret a 3D medical imaging modality end to end from the CT volume\\nto text.\\nUsing the same human evaluation rubrics introduced in Section 4.3, we evaluated a total of 92\\nnon-contrast head/neck CT studies consisting of 27 Normal-labeled cases without findings and 65\\nAbnormal-labeled cases that contained findings, including both acute findings such as cerebrovascular\\naccidents as well as findings that are common consequences of aging, such as atrophy. Studies\\nwere initially divided into normal and abnormal candidates based on the length of the impression\\nsections of the reports. A random subset within each was selected and then manually classified by a\\nboard-certified radiologist into the normal or abnormal category based on the full radiology report.\\nStudies classified as normal contained no findings.\\nIn reviewing the reports, a single academic board-certified examined the study and all series using\\na web-based Picture Archiving and Communication System (PACS) viewer. The radiologist graded\\nthe two reports using the same rubric presented for evaluating CXR reports. For each rating, the\\nradiologist also recorded a comment describing why the rating was given. The model generated the\\nreport based on a single series with the most slices and did not have access to any of the other series.\\nThe model was given the patient history in the form of text during inference.\\nResultsareshowninFigure4andTable10. Wefoundthat45%ofAIreportsonnormalstudiesand\\n57% of AI reports on abnormal studies would have resulted in the correct clinical management of the\\npatient, though some of those AI reports included errors that would not directly affect management.\\nWe did find, however, that only 17% of AI reports were considered to be of equivalent or better quality\\nthan the original radiologist reports. In examining the notes on errors from the generated reports,\\ni.e.,those that were scored B2, roughly half involved missed findings while the other half involved\\nhallucinations such as identified subdural hematomas or cysts. In terms of B1category reports,\\ncomments about the generated report mention it either incorrectly estimates or under-characterizes\\nwhite matter changes.\\nWhile our early results presented here leave significant room for future improvement, the potential\\nopportunity for AI in volumetric imaging is vast. This difficulty in reporting on volumetric data can\\nresult in concerning diagnostic delays (NHS, 2024). The ability to safely triage, expedite, and quality\\ncheck existing reports could be highly beneficial in health systems around the world.\\n21', metadata={'source': 'data\\\\med.pdf', 'page': 20}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\n0 20 40 60 80 100ALL (1449)ABNORMAL (977)NORMAL (472)AI superior, original report missed key findings.\\nBoth lead to correct outcome, AI more detailed.\\nBoth lead to correct outcome, with similar findings.\\nBoth lead to correct outcome, original more detailed.\\nOriginal report superior, AI missed key findings.\\n(a)MIMIC-CXR subset of 306 cases\\n0 20 40 60 80 100ALL (1423)ABNORMAL (945)NORMAL (478)AI superior, original report missed key findings.\\nBoth lead to correct outcome, AI more detailed.\\nBoth lead to correct outcome, with similar findings.\\nBoth lead to correct outcome, original more detailed.\\nOriginal report superior, AI missed key findings.\\n(b)IND1 subset of 300 cases\\nFigure 3|Med-Gemini-2D CXR report generation results based on 4 India-based board-certified radiologists\\nand one US-based academic board-certified radiologist performing report comparisons. (a)a subset of 306\\nMIMIC-CXR cases. Overall 48% of the cases were equal or superior to the original reports and 72% lead to the\\nsame clinical outcome. (b)a subset of 300 cases from IND1. Overall 75% of the reports were equal or superior\\nto the original reports. In both studies, the AI performance was better on the normal cases.\\nTable 10|Human evaluation results for Head CT Volume report generation Note there is no existing\\nbest-in-class performance for this task as report generation from Head/Neck 3D CT volumes is a new capability.\\nAdditionally, the model had access to a single series in the study for report generation.\\nDataset Metric Med-Gemini\\nCT-US1AI superior or similar to original report (Normal) 15%\\nAI superior or similar to original report (Abnormal) 18%\\nClinically acceptable AI generated report (All) 53%\\n4.5. Disease prediction from genetic information\\nPersonalized medicine can benefit greatly from genetics, as disease risks depend heavily on an\\nindividual’s genetic makeup. To leverage this powerful information, we expanded our model’s ability\\nto process genetic information in the form of an RGB image by featurizing the genome into polygenic\\nrisk scores (PRS) as explained in Section 2.\\nTo assess the disease risk prediction capability of Med-Gemini-Polygenic, we created benchmarks\\nby training linear models on all PRS featurizations plus demographics (“Ensemble of PRSs and\\ndemographics”) which is the current best practice for using PRSs for disease prediction (Albiñana\\net al., 2023; Truong et al., 2024). For in-distribution health outcomes (see Section 2) used in Med-\\nGemini-Polygenic training, we directly applied the trained “Ensemble of PRSs and demographics”\\n22', metadata={'source': 'data\\\\med.pdf', 'page': 21}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\n0 20 40 60 80 100ALL (92)ABNORMAL (65)NORMAL (27)AI superior, original report missed key findings.\\nBoth lead to correct outcome, AI more detailed.\\nBoth lead to correct outcome, with similar findings.\\nBoth lead to correct outcome, original more detailed.\\nOriginal report superior, AI missed key findings.\\nFigure 4|Med-Gemini-3D Head CT report generation scores on a subset of 92 cases from the CT-US1 test set\\nscored by a US-based board certified radiologist. Across all cases, 17% of the AI generated reports were graded\\nequivalent or superior to that of radiologists’ reports, while 53% were judged as resulting in equivalent patient\\ncare. Performance was better overall on abnormal versus normal cases.\\nmodels as benchmarks. For health outcomes that were never used in the training process (out-\\nof-distribution or OOD, see Section 2) but share some genetic correlation with the in-distribution\\noutcomes, we first calculated their phenotypic correlations with all the in-distribution outcomes, and\\nused the model trained to predict the most correlated in-distribution outcome to generate a maximally\\nstrong performance benchmark.\\nWe evaluated Med-Gemini-Polygenic performance on case/control balanced datasets sampled\\nfrom the test split (200 cases, 200 controls per outcome) for computational efficiency (Section A.2.2).\\nWe obtained a disease probability score from Med-Gemini-Polygenic by prompting it to predict the\\nstatus of a given health outcome using a text prompt and the genetic risk “image” (Table A.9), and\\ncomputedtheprobabilityastheratioofthelikelihoodsofthemodelgeneratingapositiveandnegative\\nprediction. Med-Gemini-Polygenic achieved higher AUCs than the PRS linear model benchmarks for\\nall in-distribution health outcomes except glaucoma (Figure 5). To evaluate zero-shot generalization\\nability, we prompted Med-Gemini-Polygenic to predict disease status for the six out-of-distribution\\nhealth outcomes. Med-Gemini-Polygenic achieved similar performance to benchmarks trained on\\nthe most correlated in-distribution outcome (Table A.11) despite never being instructed about the\\nassociations between in-distribution and out-of-distribution outcomes (Figure 5).\\nAdditionally, we compared the performance of linear probes of the Med-Gemini-Polygenic em-\\nbeddings and directly prompting Med-Gemini-Polygenic in the evaluation sets of 400 individuals.\\nComparisons of the AUCs show that while Med-Gemini-Polygenic performs similarly to the linear\\nprobe when each is only given demographic information, it often outperforms the linear probe when\\nincorporating both PRSs and demographics (Figure A.5). This performance increase is largely at-\\ntributable to Med-Gemini-Polygenic modeling non-linear interactions between genomic information\\nand demographics (Table A.12).\\nWe caution that the AUC values reported here represent an upper bound on model performance\\nsince the GWASs used to create the PRS features were performed within the UK Biobank. However,\\nthe relative performance of different models that all operate on this in-sample data is the measure of\\ninterest for these analyses.\\n5. Qualitative Results\\nIn this section we provide a few examples showcasing our model’s capability in medical dialogue for\\ndiverse set of medical modalities including chest X-ray, CT, fundus, dermatology, pathology, depicted\\nin Figures 6, 7 as well as 2D (Figure 8) and 3D (Figure 9) radiology report generation. As highlighted\\n23', metadata={'source': 'data\\\\med.pdf', 'page': 22}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nMajor\\nDepressionStrokeGlaucomaRheumatoid\\n      ArthritisAll-cause\\n     MortalityCoronary\\nArtery DiseaseCOPDType\\n2 Diabetes405060708090AUCIn-distribution\\nHyperchol-\\nesterolemiaAsthmaPneumoniaHyper-\\ntension Atrial\\n          FibrillationDiabetic\\n     Retinopathy405060708090AUCOut-of-distribution\\nDemographics only Ensemble of PRSs & Demographics Med-Gemini (PRSs & demographics)\\nFigure 5|Health outcome prediction using Med-Gemini-Polygenic compared to two baselines for both in-\\ndistribution and out-of-distribution outcomes. “Demographics only” used a linear probe of age,sex, and\\nBMIto predict each health outcome, and “Ensemble of PRSs and demographics” combined demographics\\nwith all 7,145 PRSs in a linear probe. Med-Gemini-Polygenic was prompted with both an individual’s PRS\\nimage and demographics. For out-of-distribution health outcomes, the linear probes (“Ensemble of PRSs and\\ndemographics” and “Demographics only”) were trained to predict the most-correlated in-distribution outcome\\n(Table A.11), and those predictions were then evaluated on the out-of-distribution outcome.\\nin these examples, Med-Gemini is able to provide accurate and reasonable multimodal dialogue\\nand interpretation capabilities across a variety of medical imaging domains. At the same time,\\nexpert review of these examples highlights areas for improvement regarding the phrasing, accuracy,\\nappropriate level of detail, and completeness of generated responses.\\nIn addition to understanding automated report generation capabilities, it is important to consider\\nplausible real world assistive use cases. As a proof of concept, we experimented with directing the\\nmodel’s attention to a specific region/organ within the CXR (Figure 10).\\nLastly, as shown in the above examples, even though Med-Gemini was only fine-tuned with data\\ndirectlyrelatedtoimageinterpretation(e.g. therewerenoquestion-answerpairsrelatedtotreatments\\nor symptoms in the fine-tuning set), Med-Gemini can still leverage the medical knowledge from\\nGemini pretraining to give simple but reasonable answers to those questions. While we emphasize that\\nreal-world medical diagnosis, prognosis, and treatment information is much more complicated and\\nnuanced than the examples provided here, these examples serve as a proof of concept for combining\\nlarge model pretraining with domain specialization, an active area for further improvements.\\n24', metadata={'source': 'data\\\\med.pdf', 'page': 23}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nWhat is the most likely disease in the image below? \\nResponse \\nPrompt \\nLung adenocarcinoma \\nHow to treat it? Explain in detail. \\nPrompt \\nResponse \\nLung adenocarcinoma is treated with surgery , \\nchemotherapy , and radiation therapy . \\nWhat type of symptoms would the patient be likely to \\nexperience? \\nPrompt \\nResponse \\nThe patient may experience a cough, shortness of breath, \\nchest pain, and fever. \\nThere is lung cancer, but couldn’t conﬁdently tell the \\nsubtype (adenocarcinoma) from the image alone. Other \\nthan that , the rest of the conversation is reasonable. Clinician Review \\nIs there pleural effusion in this image? Answer Y es or No. \\nResponse \\nY es. \\nHow to treat it? \\nResponse \\nPleural effusion is a condition in which ﬂuid accumulates in \\nthe space between the lungs and the chest wall. This can \\ncause difﬁculty breathing and other symptoms. T reatment \\nfor pleural effusion typically involves removing the excess \\nﬂuid through a procedure called thoracentesis. \\nPrompt \\nPrompt \\nThe model responses are reasonable. Clinician Review \\nWhat are the ﬁndings? \\nPrompt \\nResponse \\nResponse \\nHard exudates is present. Hemorrhage is present. \\nDoes the patient have DR? If yes, what is the severity level? \\nPrompt \\nY es, moderate Describe the image below. \\nPrompt \\nResponse \\nThe image shows a red, raised, and round skin lesion. The \\nmost common cause of this condition is basal cell \\ncarcinoma. \\nWhat is the treatment of this carcinoma?  \\nPrompt \\nResponse \\nBasal cell carcinoma is a type of skin cancer that is usually \\ntreated with surgery . The type of surgery used will depend \\non the size and location of the tumor. \\nResponse How to treat it? \\nPrompt \\nLaser photocoagulation \\nMissed microaneurysms in the ﬁndings. DR severity is \\ncorrect. T reatment should include Intravitreal anti-VEGF \\ninjection. Clinician Review \\nNeed to change “the most common cause” to “a common \\ncause” and “treated with surgery” to “treated with surgical \\nremoval”. Clinician Review \\nFigure 6|Example of 2D medical image dialogue via open ended question answering. For chest X-\\nray (Johnson et al., 2019a), lung CT (Liu et al., 2021), fundus images (Cuadros and Bresnick, 2009), and skin\\nlesion images (Pacheco et al., 2020).\\n25', metadata={'source': 'data\\\\med.pdf', 'page': 24}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nT ell me about this histopathology image. \\nPrompt \\nResponse \\nH&E histopathology image demonstrating invasive breast \\ncarcinoma with moderate nuclear pleomorphism (score 2). \\nThanks, what about the tubule formation? \\nPrompt \\nResponse \\nThis invasive breast carcinoma shows a scarcity of \\nwell-deﬁned glandular structures, resulting in a tubule \\nformation score of 3. T ell me about this histopathology image. \\nResponse \\nPrompt \\nH&E stained section of a lymph node demonstrating the mix \\nof small, mature lymphocytes and occasional larger immune \\ncells. \\nIs there any tumor in this image? \\nPrompt \\nResponse \\nNo. \\nWhat are the slightly larger cells in the middle left part of \\nthe image? \\nPrompt \\nResponse \\nThese are likely immune cells called plasma cells, which are \\na normal part of the immune system.  \\nAligns with the ground truth, although perhaps somewhat \\nchallenging to provide NP score at this magniﬁcation. Also, \\nother aspects of tumor grade or subtype are not \\nmentioned. Expert Review Patch-level Ground Truth \\nInvasive breast cancer, nuclear pleomorphism score 2 \\nAligns with the ground truth, although perhaps challenging \\nto be sure this is from lymph node without additional \\ncontext. Expert Review Patch-level Ground Truth \\nL ymph node, non-tumor  \\nT ell me about this image. \\nPrompt \\nResponse \\nH&E histopathology image demonstrating Gleason pattern 4 \\nprostate cancer within a prostate biopsy specimen. T ell me about this histopathology image. \\nPrompt \\nResponse \\nLung adenocarcinoma, papillary subtype, showing branching \\nprojections lined by cuboidal to columnar tumor cells. \\nPatch-level Ground Truth \\nProstate cancer, Gleason pattern 4 Patch-level Ground Truth \\nLung adenocarcinoma, papillary subtype \\nFigure7|Examplesofhistopathologyimage-baseddialogue. Theseexampleshighlightaccuratehistopathol-\\nogy interpretation and communication of information across a range of tissues and findings with only a small\\namount of visual context. While there is some promising initial evidence for ability to interact and reason\\nfurther about images (top right), current capabilities for follow up interaction and exploration of input images\\nare limited and this remains an active area for improvement. This demonstration utilizes histopathology patches\\nfrom TCGA, CAMELYON16, and other data sources as described in Lai et al. (2023).\\n26', metadata={'source': 'data\\\\med.pdf', 'page': 25}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nGiven the following chest X-ray , describe the FINDINGS and \\nIMPRESSION in the image. \\nResponse \\nPrompt \\nFINDINGS: The endotracheal tube terminates 4.5 cm above \\nthe carina. an enteric tube courses below the diaphragm \\nand terminates within the stomach. There is no focal \\nconsolidation, pleural effusion, pneumothorax, or pulmonary \\nedema. The cardiomediastinal silhouette is within normal \\nlimits. IMPRESSION: Standard position of support devices. \\nNo acute cardiopulmonary process. Given the following chest X-ray , describe the FINDINGS and \\nIMPRESSION in the image. \\nResponse \\nPrompt \\nFINDINGS: The lungs are clear without focal consolidation. \\nNo pleural effusion or pneumothorax is seen. The cardiac \\nand mediastinal silhouettes are unremarkable. IMPRESSION: \\nNo acute cardiopulmonary process. \\nFINDINGS:  Tip of the endotracheal tube projects over the \\nmid thoracic trachea, approximately 3.7 cm from the \\ncarina. Enteric tube terminates beyond the diaphragm, in \\nthe left upper quadrant. Lungs are clear and \\ncardiomediastinal silhouette is normal. IMPRESSION: \\nAppropriate position of endotracheal and enteric tubes. Radiologist Written Report \\nFYI: 10522265/55255109 FINDINGS: Frontal and lateral views of the chest were \\nobtained. No focal consolidation, pleural effusion or \\nevidence of pneumothorax is seen. The cardiac and \\nmediastinal silhouettes are unremarkable. No pulmonary \\nedema is seen. IMPRESSION: No acute cardiopulmonary \\nprocess. Please note that PCP may be radiographically \\noccult. Radiologist Written Report \\nFYI: 12977138/50332797 \\nGiven the following chest X-ray , describe the FINDINGS and \\nIMPRESSION in the image. \\nResponse \\nPrompt \\nFINDINGS: There is a right lower lobe consolidation. there is \\nalso a small right pleural effusion. The heart is mildly \\nenlarged. There is mild pulmonary vascular congestion. \\nthere is no pneumothorax. IMPRESSION: right lower lobe \\npneumonia. Small right pleural effusion. Given the following chest X-ray , describe the FINDINGS and \\nIMPRESSION in the image. \\nResponse \\nPrompt \\nFINDINGS: The lungs are hyperinﬂated with ﬂattening of the \\ndiaphragms and increased ap diameter, suggesting chronic \\nobstructive pulmonary disease. No focal consolidation, \\npleural effusion, or pneumothorax is detected. Heart and \\nmediastinal contours are within normal limits with extensive \\naortic calciﬁcation. IMPRESSION: no acute cardiopulmonary \\nprocess. \\nFINDINGS: P A and lateral chest radiographs were obtained. \\nThere is an ill-deﬁned opacity in the right lower lobe that \\ndoes not obscure the right heart border. A right-sided \\npleural effusion is small. There is no pneumothorax. \\nCardiomegaly is mild. Aortic calciﬁcations are minimal. \\nIMPRESSION: Right lower lobe pneumonia and small right \\npleural effusion. Discussed with Dr ___ ___ phone at ___. Radiologist Written Report \\nFYI: 16698318/55141338 FINDINGS: Frontal and lateral views of the chest were \\nobtained. The lungs are hyperinﬂated, with ﬂattening of the \\ndiaphragms, consistent with chronic obstructive pulmonary \\ndisease. There is mild left basilar atelectasis. No focal \\nconsolidation is seen. There is no pleural effusion or \\npneumothorax. The cardiac and mediastinal silhouettes are \\nunremarkable. Degenerative changes are seen along the \\nspine. IMPRESSION: COPD and left basilar atelectasis. \\nOtherwise, no acute cardiopulmonary process. Radiologist Written Report \\nFYI: 12977138/50332797 \\nFigure 8|Examples of chest X-ray report generation. These examples demonstrate the capability of Med-\\nGemini for CXR report generation on various conditions. Top-left, support devices; Top-right, normal case;\\nBottom-left, acute abnormality; Bottom-right, chronic abnormality. A radiologist reviewed all these examples\\nand confirmed model generated reports are reasonable with one note for the bottom-right case where the\\n“increased ap diameter” is usually detected from a lateral image, and thus a hallucination from the model.\\n27', metadata={'source': 'data\\\\med.pdf', 'page': 26}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nGiven the image, what are the radiology ﬁndings? \\nResponse \\nPrompt \\nFINDINGS: mild cortical atrophy and periventricular white matter changes are noted compatible with microvasculopathy . there is \\nproportionate ventriculomegaly . no bleed, mass effect , midline shift or abnormal intracranial ﬂuid is seen. no evidence of acute \\nintracranial hemorrhage, territorial infarct , abnormal ﬂuid or mass effect.no abnormal extra-axial ﬂuid collection is identiﬁed. no \\ndeﬁnitive evidence of intracranial hemorrhage or large territorial infarct within the limits of ct imaging. old left basilar ganglia  \\ninfarct . no intracranial mass, swelling, hemorrhage, infarction or extra-axial collection is seen. visualized mastoid air cells: well \\npneumatized. visualized paranasal sinuses: well pneumatized. atherosclerotic calciﬁcation of vasculature at the skull base. \\nFINDINGS: the fourth ventricle is midline. the remaining ventricular system is within normal limits. there is mild chronic white \\nmatter microangiopathic disease and moderate generalized cortical atrophy . no bleed, mass effect , midline shift or abnormal \\nintracranial ﬂuid is seen. bone windows demonstrate an intact skull. the remaining visualized paranasal sinuses and mastoids are \\nclear.  chronic white matter microangiopathic disease, mild generalized cortical atrophy , not signiﬁcantly changed since 2015. no \\nevidence of acute intracranial hemorrhage, territorial infarct , abnormal ﬂuid or mass effect. Radiologist Written Report \\n…  …  …  …  …  \\nGiven the image, what are the radiology ﬁndings? \\nResponse \\nPrompt \\nFINDINGS: there is a moderate amount of scattered low density  in the periventricular and subcortical white matter of the \\ncerebral hemispheres. there is no evidence of intracranial mass effect , acute hemorrhage, midline shift or extra-axial ﬂuid \\ncollection. no acute subarachnoid, subdural, epidural, or intraparenchymal hemorrhage is visualized. there is no abnormal \\nintracranial mass effect , midline shift , hydrocephalus, acute hemorrhage, or large infarct. no hydrocephalus, subarachnoid \\nhemorrhage, or mass. no mass, hemorrhage, or acute infarction is demonstrated. cerebellum: no edema, hemorrhage, mass, acute \\ninfarction, or inappropriate atrophy . brainstem: no edema, hemorrhage, mass, acute infarction, or inappropriate atrophy . sella: no \\nparasellar mass identiﬁed. skull: no mass or signiﬁcant visible lesion. the calvarium is intact. the mastoid air cells and middle ear \\ncavities are normally aerated. there is minimal mucosal thickening lining the right and left maxillary sinuses. minimal mucosal \\nthickening lining the right and left maxillary sinuses. \\nFINDINGS: scattered punctate periventricular and subcortical white matter hypodensities without mass effect or volume loss are \\ncompatible with mild microangiopathic white matter changes. ventricles and sulci are normal in size and conﬁguration. no \\nparenchymal hemorrhage, intra-axial or extra-axial ﬂuid collection, or mass lesion is present. no acute transcortical infarction, \\nregional mass effect , transtentorial herniation, or midline shift is present. visualized paranasal sinuses are clear. visualized \\nmastoid air cells are clear. visualized osseous labyrinth structures appear normal. visualized orbits are normal. visualized soft \\ntissues of the scalp are normal. no calvarial fracture. skull base and craniocervical junction are normal. atherosclerotic \\ncalciﬁcation tracks along the cavernous and supraclinoid internal carotid artery segments. Radiologist Written Report \\n…  …  …  …  …  \\nFigure 9|Examples of 3D Head CT report generations. These examples showcase 3D medical image\\ndialogue for Head CT report generation: (top) correct abnormal case, (bottom) incorrect abnormal case. While\\nMed-Gemini can identify some abnormalities missed by radiologist generated reports (highlighted in green), it\\ncan also mischaracterize findings that are present (highlighted in red) or hallucinate findings that are absent\\nfrom the image.\\n28', metadata={'source': 'data\\\\med.pdf', 'page': 27}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nGiven the following chest X-ray , describe the FINDINGS and \\nIMPRESSION in the image. \\nResponse \\nPrompt \\nFINDINGS: the lungs are clear without focal consolidation, \\neffusion, or edema. cardiomediastinal silhouette is stable. \\ntortuosity of the descending thoracic aorta is again noted. \\nno acute osseous abnormalities. IMPRESSION: no acute \\ncardiopulmonary process. Given the following chest X-ray , describe the FINDINGS and \\nIMPRESSION in the image. \\nResponse \\nPrompt \\nFINDINGS: there is a new right lower lobe inﬁltrate. the \\nremainder the appearance of the chest is unchanged \\nIMPRESSION: new right lower lobe inﬁltrate \\nMissing concepts of emphysema on the lung. Comparison with Radiologist’s Notes (not shown): \\nFYI: 229975a2-d2e6a791-a66a597a-9b370606-8323c2cd FYI: 87f64c4d-93ab83e7-04f10c4b-a9ed71f7-d05889f2 \\nMissing ﬁnding of pulmonary edema. Comparison with Radiologist’s Notes (not shown): \\nAuto-completed Response \\n…of the lungs is consistent with emphysema .\\nResponse Hint \\nFINDINGS: Hyperlucency of the upper zones… \\nAuto-completed Response \\n…a mild interstitial pulmonary edema ..\\nResponse Hint \\nFINDINGS: In the lung there is… \\nFigure 10|Examples of chest X-ray report autocompletion. In these examples, particular concepts were\\nmissing from the report generated without any hint, and were recovered with the autocomplete prefix hint. A)\\nEmphysema, B) Pulmonary Edema.\\n6. Related Work\\nThe evolution of medical language models Large language models (LLMs) built on Transformer\\narchitectures (Parmar et al., 2018; Vaswani et al., 2017) have seen rapid advancement, driving\\nsignificant progress in natural language processing and multimodal modeling. Pathway scaling\\nmethods (Barham et al., 2022) have been crucial in enabling the development of ever-larger models\\nlike the PaLM family including PaLM, PaLM 2, and PaLM-E (Anil et al., 2023; Chowdhery et al., 2023;\\nDriess et al., 2023). Other significant LLMs include BERT (Devlin et al., 2018), GPT family (Achiam\\net al., 2023; Brown et al., 2020; Radford et al., 2019), T5 (Raffel et al., 2020), and LLaMA (Touvron\\net al., 2023), Hyena (Poli et al., 2023), Mistral 7B (Jiang et al., 2023). LLMs are often refined through\\ntechniques like Chain of Thought (CoT) prompting (Wei et al., 2022) or fine-tuning (FLAN) (Wei\\net al., 2022).\\nThese advancements have catalyzed an expansion of LLMs specifically designed for medical\\ndomains, such as PubMedGPT (Bolton et al., 2022), BioGPT (Luo et al., 2022), Med-PaLM (Singhal\\netal.,2023a)anditssuccessorMed-PaLM2(Singhaletal.,2023b), ClinicalCamel(Tomaetal.,2023),\\nMedAlpaca (Han et al., 2023), BioMistral (Labrak et al., 2024), LLMs for clinical trial recruitment\\n(Wornow et al., 2024), and others. Language models can handle omic information, as demonstrated\\nby models such as HyenaDNA (Nguyen et al., 2024), BioT5 (Pei et al., 2023), sc-GPT (Cui et al.,\\n2024), and ProtLLM (Zhuo et al., 2024).\\nMultimodal models in medicine Beyond language and text alone, multimodal models like Flamingo\\n(Alayrac et al., 2022), PaLI (Chen et al., 2022), GPT-4 (Achiam et al., 2023), GPT-4v (OpenAI, 2023),\\nand LLaVa (Liu et al., 2023, 2024a) have demonstrated remarkable capability in processing both text\\nand images. Gemini (Gemini Team, Google, 2023; Google, 2024) introduced further advancement in\\n29', metadata={'source': 'data\\\\med.pdf', 'page': 28}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nmultimodal capabilities, exhibiting a distinct ability to reason across text, images, and other modalities\\nsuch as video and audio.\\nBuilding upon these capable generic multimodal models, for medical applications specifically,\\nrecent works include vision-language models that span multiple medical imaging modalities as\\nwell as those that focus on a specific imaging domain, such as radiology or histopathology. Efforts\\nsuch as Med-Flamingo (Moor et al., 2023b), BiomedCLIP (Zhang et al., 2023a), Med-PaLM M (Tu\\net al., 2024), BiomedGPT (Zhang et al., 2023a), Flamingo-CXR (Tanno et al., 2024), LLaVa-Med (Li\\net al., 2024), PMC-VQA (Zhang et al., 2023b), RadFM (Wu et al., 2023), ELIXR (Xu et al., 2023),\\nXrayGPT (Thawkar et al., 2023), MAIRA-1 (Hyland et al., 2023), HeLM (Belyaeva et al., 2023), M-\\nREGLE (Zhou et al., 2024), CONCH (Lu et al., 2024), PLIP (Huang et al., 2023), PathAsst (Sun et al.,\\n2024), QuiltNet-B-32 (Ikezogwo et al., 2024) and many others specifically explore the potential of\\nmultimodal models for medical applications, signaling a growing interest in this area. These methods\\ncover a range from generalist to specialist approaches. Models such as MAIRA-1 (Hyland et al., 2023),\\nXrayGPT (Thawkar et al., 2023), Radiology-GPT (Liu et al., 2024b), and CT2Rep (Hamamci et al.,\\n2024) focus on radiology report generation, and among modalities choose only chest X-ray or chest\\nCT report generation. Some of these approaches broaden their capabilities to cover multiple types of\\nmodalities but focus on only one task, such as methods that aim for VQA capabilities like LLaVA-Med\\n(Li et al., 2024), and PMC-VQA (Zhang et al., 2023b), aiming to build assistants for medical question\\nanswering.\\nWhile specialized VLMs demonstrate particular strengths, generalist models capable of handling\\na wide range of tasks and modalities, such as Med-PaLM M, are gaining prominence. The field of\\nmedical AI is witnessing the emergence of comprehensive ‘Generalist Medical AI’ models (Moor et al.,\\n2023a,b; Tu et al., 2024; Zhang et al., 2023a) and the orchestration of AI tools for medical tasks\\nusing LLMs (Ferber et al., 2024). These models aspire to provide robust interaction with medical\\ninformation in a manner similar to what general-purpose LLMs have done for broader domains.\\nPioneering efforts like these offer important initial insights into the potential for large multimodal\\nmodels to provide assistance across various medical tasks using a unified platform. This inconsistency\\nunderscores the urgent need for a unified benchmark to enable meaningful evaluation in this rapidly\\nevolving field.\\nMultimodal evaluation benchmark and metrics Evaluation of medical VLMs suffers from a lack of\\nconsistency and standardization, creating a new landscape for works proposing new benchmarks\\nto fill this gap. Multiple recent works demonstrate this inconsistency with varying tasks, datasets,\\nand completely distinct sets of metrics, hindering direct comparison even for a same dataset. Along\\nthese lines, multiple recent works (Fleming et al., 2023; Moor et al., 2023b; Royer et al., 2024; Tu\\net al., 2024; Wu et al., 2023) suggest multimodal benchmarks such MultiMedEval, MultiMedBench,\\nRadBench, RadMD, and MedMD to evaluate these generalist and multimodal models in a more\\nsystematic fashion.\\n7. Discussion\\nIn this study, we present three new models within the Med-Gemini family, based upon Gemini 1.5,\\nacrossvariousmedicalmodalities. Weshowpromisingperformanceacrossanumberoftasks,including\\nclassification, VQA, and report generation. Our Med-Gemini models are able to process complex\\nmedical data types, including 2D and 3D radiology images, histopathology patches, ophthalmology\\nimages, dermatology images, and genetic risk scores. Importantly, our models were fine-tuned using\\npredominantly medical data and paired free text descriptive reports. These reports are ubiquitous in\\nhealthcare and our ability to use them as a training objective reduces the need for further expensive\\nexpert labelling.\\nThe results in this study show early potential across a number of different tasks and individual\\n30', metadata={'source': 'data\\\\med.pdf', 'page': 29}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nmodalities. Webelievethatthecombinationoftasksandmultiplemodalitiesinfutureworkwillenable\\nAI models to address a far wider range of applications than has been previously possible. Longer\\ncontext windows and improved reasoning abilities will enable decision-making that incorporates\\nhistorical context, more closely reflecting how human specialists operate.\\nThe opportunity for LMMs to analyze complex medical types including 3D radiology and large\\npathology images presents an exciting range of potential downstream applications. This work\\nshowcases our early explorations in CT, a three-dimensional modality that has been challenging to\\nintegrate with LMMs to date. This is due to a combination of vast data size, architectural limitations,\\nand the jump in clinical task complexity of interpreting 3D imaging modalities ( vs.2D). While our\\nresults are currently a proof of concept, and do not yet reach performance required for clinical use,\\nwe expect architectures to rapidly improve. We look forward to exploring other similar complex\\nmodalities in future work.\\nWhile our findings in this study are promising and provide a glimpse into the potential of LMMs\\nin medicine, it is important to thoroughly test them beyond traditional academic benchmarks. This is\\nnecessarytoensuretheyaresafeandreliablebeforeconsideringuseinreal-worldsituations, especially\\nin safety critical areas like healthcare. In this work, we have tried to go deeper into the nuance of\\nmedicalevaluationthroughtheuseofpanelsofspecialiststoassessandratetheperformanceofmodels\\non tasks such as report generation and question answering. We believe that an increasingly diverse\\nrange of healthcare professionals need to be deeply involved in future iterations of this technology,\\nhelping to guide the models towards capabilities that have valuable real world utility. There are a\\nnumber of areas on which future evaluations should focus before models like these are considered\\nsafe and effective for clinical use:\\nClosing the gap between benchmark and bedside Despite the potential of machine learning in\\nhealthcare, there is growing concern about the reliability of algorithm validation methods. In medical\\nimageanalysis,improvementonsimplebenchmarkperformancemetricsmaynottranslatetoimproved\\noutcomes in clinical settings, leading to a disconnect between expectations and real-world usefulness.\\nBenchmark datasets are an important step towards developing clinically useful models, but given\\ntheir limitations in size, scope, and reflection of real world distributions, they are not themselves a\\nproxy for real-world performance. The potential for generative AI lies foremost in assisting, rather\\nthan replacing human specialists in the diagnosis and management of disease; evaluations should\\nshift from static benchmarks to realistic clinical scenarios that assess AI-human collaboration and its\\nimpact on patient outcomes.\\nIdentifying and mitigating data bias and safety risks LLMs and LMMs trained on vast datasets risk\\ninheriting biases and errors from their source data. This can lead to misdiagnoses and amplification\\nof systemic bias. Before models like these are used in real world settings, careful evaluations that\\naddress safety and bias risks should be performed and any discovered risks should be mitigated (Weng\\net al., 2024). End users should also carefully validate model performance for their specific use cases\\nand patient populations.\\nMinimizingdatacontaminationwhenevaluatingzero-shotgeneralizationinlargemodels While\\nLLMs exhibit impressive zero-shot generalization, it’s important to note that their massive training\\ndatasets increase the potential for data contamination, which may result in overestimation of their\\ntrue generalization abilities. Large models like Gemini might have inadvertently “seen” examples\\nrelated to the task during training, even if those examples were not explicitly labeled. This hidden\\nexposure compromises our understanding of models’ true ability to generalize to completely novel\\nconcepts when evaluating on open datasets. Researchers are actively investigating the impact of data\\ncontamination to ensure we accurately gauge capabilities of such large models (Udandarao et al.,\\n2024; Vogel et al., 2022). Prospective studies, while typically more expensive and time-consuming to\\nexecute than retrospective studies, are another option for mitigating this risk.\\n31', metadata={'source': 'data\\\\med.pdf', 'page': 30}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\n8. Conclusion\\nMultimodal generative AI, exemplified by powerful models like Gemini, holds great potential to\\nrevolutionize healthcare. While medicine is a rapidly growing use case for these new models, general\\npurpose models may not naturally perform well in the medical domain due to its highly specialized\\ndata.\\nTo explore the potential for models like Gemini in medicine, we developed several models within\\nthe new Med-Gemini family, a series of models built upon the multimodal foundation of Gemini and\\nfine-tuned on a diverse range of medical data including radiology, histopathology, ophthalmology,\\ndermatologyandgenomics. WeassessedourMed-Geminimodels’performanceusingacomprehensive\\nmedicalbenchmarkingsuite,includingbothestablishedbenchmarksandcustombenchmarksdesigned\\nto reflect clinical relevance. Notably, some benchmarks involved evaluations by medical experts for\\ntasks such as generating CXR and CT reports and radiology VQA.\\nMed-Gemini-2D sets a new standard for expert-evaluated chest X-ray report generation, outper-\\nforming previous models, and Med-Gemini-3D showcases the first LMM-based report generation for\\n3D CT. Beyond report generation, Med-Gemini-2D demonstrates exceptional performance in VQA\\nand classification across various medical imaging modalities. Beyond imaging, Med-Gemini-Polygenic\\noutperforms conventional polygenic risk score methods in predicting disease risk. These results\\ndemonstrate the potential of the Gemini foundation and the fine-tuned Med-Gemini family in the\\nmedical domain. Nonetheless, the results also underscore the need for further rigorous research to\\nensure safe and effective implementation in real-world clinical settings.\\nWhile advanced capabilities on individual medical tasks are useful in their own right, we envision\\na future in which all of these capabilities are integrated together into comprehensive systems to\\nperform a range of complex multidisciplinary clinical tasks, working alongside humans to maximize\\nclinical efficacy and improve patient outcomes. The results presented in this study represent a step\\ntowards realizing this vision.\\n32', metadata={'source': 'data\\\\med.pdf', 'page': 31}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\n9. Contributions and Acknowledgments\\nContributions\\nAuthors are listed here associated with their primary workstreams. Many authors contributed to\\nadditional workstreams beyond the one under which they are listed.\\nTechnical Leads\\nLin Yang∗,1\\nShawn Xu∗,1\\nAndrew Sellergren*,1\\n∗Equal contribution\\nChest X-Ray\\nTimo Kohlberger1\\nIra Ktena2\\nKendall Park1\\nRyutaro Tanno2\\nDavid G. T. Barrett2\\nWei-Hung Weng1\\nKhaled Saab1\\nTao Tu1\\nComputed Tomography\\nAtilla Kiraly1\\nAkshay Goel1\\nArnav Agharwal1\\nNick George1\\nGenomics\\nCory Y. McLean1\\nYuchen Zhou1\\nFarhad Hormozdiari1\\nEric Wang2\\nPathology\\nDave Steiner1\\nFaruk Ahmed1\\nTiam Jaroensri1\\nEllery Wulczyn1\\nAdditional Contributions\\nFayaz Jamil1\\nTheo Guidroz1\\nYang Wang3\\nSiyuan Qiao2\\nYun Liu1\\nS. Sara Mahdavi2\\nClinical\\nCharles Lau4\\nSreenivasa Raju Kalidindi5\\nMozziyar Etemadi6\\nJorge Cuadros7\\nGregory Sorensen8Google Research and Google DeepMind\\nLeadership\\nShruthi Prabhakara1\\nDaniel Tse1\\nShravya Shetty1\\nGreg Corrado1\\nKatherine Chou1\\nYossi Matias1\\nS. M. Ali Eslami2\\nDavid Fleet2\\nJoelle Barral2\\nResearch Leads\\nDaniel Golden†,1\\nShekoofeh Azizi†,2\\nRory Pilgrim1\\nChristopher Kelly1\\n†Equal contribution\\n1Google Research\\n2Google DeepMind\\n3Verily Life Sciences\\n4Google Research via Advanced Clinical\\n5Apollo Radiology International\\n6Northwestern Medicine\\n7EyePACS, Inc and Meredith Morgan University\\nEye Center, University of California at Berkeley\\n8DeepHealth / RadNet\\n33', metadata={'source': 'data\\\\med.pdf', 'page': 32}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nAcknowledgements\\nThis project was an extensive collaboration between many teams at Google Research and Google\\nDeepMind. We thank Kevin Swersky and Mike Schaekermannn for their feedback and insight, which\\nsignificantly contributed to the enhancement of this report. We also thank Sami Lachgar, Lauren\\nWiner, Maggie Shiels, Jessica Valdez, Jon Small, Aaron Abood, Rishad Patel, Christian Wright, Annisah\\nUm’rani, Jean-baptiste Alayrac, Aishwarya Kamath, Viorica Patraucean, Rory Sayres, Abbi Ward,\\nLouis Blankemeier, Olga Kanzheleva, Taedong Yun, Ksenia Konyushkova, Christos Kaplanis, Juanma\\nZambranoChaves,AlanKarthikesalingam,VivekNatarajan,andCanKirmizifortheirvaluableinsights,\\ntechnical support and feedback during our research. We thank Kimberly Kanada and Ilana Traynis for\\ntheir review of the qualitative examples shown in this manuscript. We are grateful to Jonathon Shlens,\\nDale Webster and Oriol Vinyals for their support during the course of this project. We also thank\\nMichael Colligan and Brittany Stein from DeepHealth/RadNet for their support with data curation.\\nThis research was conducted using the UK Biobank Resource under application number 65275.\\nThe results shown here are in part based upon data generated by the TCGA Research Network. The\\nauthors thank the National Cancer Institute for access to NCI’s data collected by the National Lung\\nScreening Trial (NLST). The statements contained herein are solely those of the authors and do not\\nrepresent or imply concurrence or endorsement by NCI.\\nData Availability\\nExcept IND1, CXR-US2, and CT-US1, Eyepacs, and TTH, which are private datasets, the rest of the\\ndatasets utilized for developing, benchmarking, and evaluation of Gemini and Med-Gemini in this\\nreport are publicly accessible with appropriate permissions. We intend to publicly release our updated\\nclassification labels and custom VQA question and answer pairs for the MIMIC-CXR dataset, our splits\\nfor the PAD-UFES-20 and VQA-Rad datasets, and several suggested replacement question and answer\\npairs for the VQA-Rad dataset which were recommended by our reading radiologist. This text will be\\nupdated when that data is available.\\nCode Availability\\nWe will not open-source the model code and weights because of the safety concerns associated with\\nunmonitored use in medical settings. To ensure responsible innovation, we will collaborate with our\\nresearch partners and healthcare providers to validate and explore safe applications of the Gemini\\nand Med-Gemini through Google Cloud APIs.\\nCompeting Interests\\nThis study was funded by Alphabet Inc and/or a subsidiary thereof (‘Alphabet’). Authors who are\\naffiliated with Google Research, Google DeepMind, and Verily Life Sciences are employees of Alphabet\\nand may own stock as part of the standard compensation package.\\nUse of AI in Manuscript Preparation\\nThis manuscript was written manually, with a small number of copy edits performed using Gemini.\\nThe authors take all responsibility for the contents.\\n34', metadata={'source': 'data\\\\med.pdf', 'page': 33}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\\narXiv preprint arXiv:2303.08774 , 2023.\\nJulián N Acosta, Guido J Falcone, Pranav Rajpurkar, and Eric J Topol. Multimodal biomedical ai.\\nNature Medicine , 28(9):1773–1784, 2022.\\nJong Seok Ahn, Shadi Ebrahimian, Shaunagh McDermott, Sanghyup Lee, Laura Naccarato, John F\\nDi Capua, Markus Y Wu, Eric W Zhang, Victorine Muse, Benjamin Miller, et al. Association of\\nartificial intelligence–aided chest radiograph interpretation with reader performance and efficiency.\\nJAMA Network Open , 5(8):e2229289–e2229289, 2022.\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model\\nfor few-shot learning. Advances in neural information processing systems , 35:23716–23736, 2022.\\nClara Albiñana, Zhihong Zhu, Andrew J. Schork, Andrés Ingason, Hugues Aschard, Isabell Brikell,\\nCynthia M. Bulik, Liselotte V. Petersen, Esben Agerbo, Jakob Grove, et al. Multi-pgs enhances\\npolygenic prediction by combining 937 polygenic scores. Nature Communications , 14(4702), 2023.\\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint\\narXiv:2305.10403 , 2023.\\nDiegoArdila,AtillaP.Kiraly,SujeethBharadwaj,BokyungChoi,JoshuaJ.Reicher,LilyPeng,DanielTse,\\nMozziyar Etemadi, Wenxing Ye, Greg Corrado, David P. Naidich, and Shravya Shetty. End-to-end\\nlung cancer screening with three-dimensional deep learning on low-dose chest computed tomogra-\\nphy.Nature Medicine , 25(6):954–961, 2019. ISSN 1546-170X. doi: 10.1038/s41591-019-0447-x.\\nURLhttps://doi.org/10.1038/s41591-019-0447-x .\\nShekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron\\nLoh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, et al. Big self-supervised models advance\\nmedical image classification. In Proceedings of the IEEE/CVF international conference on computer\\nvision, pages 3478–3488, 2021.\\nShekoofehAzizi, LauraCulp, JanFreyberg, BasilMustafa, SebastienBaur, SimonKornblith, TingChen,\\nNenad Tomasev, Jovana Mitrović, Patricia Strachan, et al. Robust and data-efficient generalization\\nof self-supervised machine learning for diagnostic imaging. Nature Biomedical Engineering , 7(6):\\n756–779, 2023.\\nShruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel C\\nCastro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al. Learning to exploit\\ntemporal structure for biomedical vision-language processing. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pages 15016–15027, 2023.\\nPaulBarham,AakankshaChowdhery,JeffDean,SanjayGhemawat,StevenHand,DanielHurt,Michael\\nIsard,HyeontaekLim,RuomingPang,SudipRoy,etal. Pathways: Asynchronousdistributeddataflow\\nfor ml.Proceedings of Machine Learning and Systems , 4:430–449, 2022.\\nBabak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karsse-\\nmeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka\\nBalkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of lymph node\\nmetastases in women with breast cancer. Jama, 318(22):2199–2210, 2017.\\n35', metadata={'source': 'data\\\\med.pdf', 'page': 34}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nAnastasiya Belyaeva, Justin Cosentino, Farhad Hormozdiari, Krish Eswaran, Shravya Shetty, Greg\\nCorrado, Andrew Carroll, Cory Y McLean, and Nicholas A Furlotte. Multimodal llms for health\\ngrounded in individual-specific data. In Workshop on Machine Learning for Multimodal Healthcare\\nData, pages 86–102. Springer, 2023.\\nAsma Ben Abacha, Mourad Sarrouti, Dina Demner-Fushman, Sadid A. Hasan, and Henning Müller.\\nOverview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation\\nin the Medical Domain. In CLEF 2021 Working Notes , CEUR Workshop Proceedings, Bucharest,\\nRomania, September 21-24 2021. CEUR-WS.org.\\nElliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy\\nLiang. Stanford crfm introduces pubmedgpt 2.7b. https://hai.stanford.edu/news/\\nstanford-crfm-introduces-pubmedgpt-27b , 2022.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot\\nlearners. Advances in neural information processing systems , 33:1877–1901, 2020.\\nClare Bycroft, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T Elliott, Kevin Sharp, Allan\\nMotyer, Damjan Vukcevic, Olivier Delaneau, Jared O’Connell, Adrian Cortes, Samantha Welsh, Alan\\nYoung, Mark Effingham, Gil McVean, Stephen Leslie, Naomi Allen, Peter Donnelly, and Jonathan\\nMarchini. The UK biobank resource with deep phenotyping and genomic data. Nature, 562(7726):\\n203–209, October 2018.\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLi: A jointly-scaled multilingual\\nlanguage-image model. arXiv preprint arXiv:2209.06794 , 2022.\\nZhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating radiology reports via\\nmemory-driven transformer. arXiv preprint arXiv:2010.16056 , 2020.\\nShing Wan Choi, Timothy Shin-Heng Mak, and Paul F O’Reilly. Tutorial: a guide to performing\\npolygenic risk score analyses. Nature Protocols , 15(9):2759–2772, 2020.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling\\nlanguage modeling with pathways. Journal of Machine Learning Research , 24(240):1–113, 2023.\\nJustin Cosentino, Babak Behsaz, Babak Alipanahi, Zachary R McCaw, Davin Hill, Tae-Hwi Schwantes-\\nAn, Dongbing Lai, Andrew Carroll, Brian D Hobbs, Michael H Cho, et al. Inference of chronic\\nobstructive pulmonary disease with deep learning on raw spirograms identifies new genetic loci\\nand improves risk models. Nature Genetics , 55(5):787–795, 2023.\\nJorge Cuadros and George Bresnick. Eyepacs: an adaptable telemedicine system for diabetic retinopa-\\nthy screening. Journal of diabetes science and technology , 3(3):509–516, 2009.\\nHaotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. scGPT:\\ntoward building a foundation model for single-cell multi-omics using generative ai. Nature Methods ,\\npages 1–11, 2024.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\\nlanguage model. arXiv preprint arXiv:2303.03378 , 2023.\\n36', metadata={'source': 'data\\\\med.pdf', 'page': 35}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nMark Endo, Rayan Krishnan, Viswesh Krishna, Andrew Y Ng, and Pranav Rajpurkar. Retrieval-based\\nchest x-ray report generation using a pre-trained contrastive language-image model. In Machine\\nLearning for Health , pages 209–219. PMLR, 2021.\\nDyke Ferber, Omar SM El Nahhas, Georg Wölflein, Isabella C Wiest, Jan Clusmann, Marie-Elisabeth\\nLeßman, Sebastian Foersch, Jacqueline Lammert, Maximilian Tschochohei, Dirk Jäger, et al. Au-\\ntonomous artificial intelligence agents for clinical decision making in oncology. arXiv preprint\\narXiv:2404.04667 , 2024.\\nScott L Fleming, Alejandro Lozano, William J Haberkorn, Jenelle A Jindal, Eduardo P Reis, Rahul\\nThapa, Louis Blankemeier, Julian Z Genkins, Ethan Steinberg, Ashwin Nayak, et al. Medalign: A\\nclinician-generated dataset for instruction following with electronic medical records. arXiv preprint\\narXiv:2308.14089 , 2023.\\nGemini Team, Google. Gemini: A family of highly capable multimodal models. arXiv preprint\\narXiv:2312.11805 , 2023.\\nAry L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G\\nMark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank,\\nphysiotoolkit, and physionet: components of a new research resource for complex physiologic\\nsignals. circulation , 101(23):e215–e220, 2000.\\nGoogle. Google’s foundation model for Dermatology. https://github.com/Google-Health/\\nimaging-research/tree/master/derm-foundation , 2024. Accessed April 18, 2024.\\nGemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of\\ncontext. arXiv preprint arXiv:2403.05530 , 2024.\\nShivanand Gornale and Pooja Patravali. Digital knee x-ray images. Mendeley Data , 1, 2020.\\nIbrahim Ethem Hamamci, Sezgin Er, and Bjoern Menze. Ct2rep: Automated radiology report\\ngeneration for 3d medical imaging. arXiv preprint arXiv:2403.06801 , 2024.\\nTianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander\\nLöser, Daniel Truhn, and Keno K Bressem. Medalpaca–an open-source collection of medical\\nconversational ai models and training data. arXiv preprint arXiv:2304.08247 , 2023.\\nXuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. PathVQA: 30000+ Questions for\\nMedical Visual Question Answering. arXiv preprint arXiv:2003.10286 , 2020.\\nZhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, and James Zou. A visual–\\nlanguage foundation model for pathology image analysis using medical twitter. Nature medicine ,\\n29(9):2307–2316, 2023.\\nStephanieLHyland,ShruthiBannur,KenzaBouzid,DanielCCastro,MercyRanjit,AntonSchwaighofer,\\nFernando Pérez-García, Valentina Salvatelli, Shaury Srivastav, Anja Thieme, et al. Maira-1: A\\nspecialisedlargemultimodalmodelforradiologyreportgeneration. arXivpreprintarXiv:2311.13668 ,\\n2023.\\nWisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed,\\nPavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs\\nfor histopathology. Advances in Neural Information Processing Systems , 36, 2024.\\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik\\nMarklund,BehzadHaghgoo,RobynBall,KatieShpanskaya,etal. Chexpert: Alargechestradiograph\\ndataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on\\nartificial intelligence , volume 33, pages 590–597, 2019.\\n37', metadata={'source': 'data\\\\med.pdf', 'page': 36}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nSaahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre\\nChambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, et al. Radgraph: Extracting clinical\\nentities and relations from radiology reports. arXiv preprint arXiv:2106.14463 , 2021.\\nRonnachai Jaroensri, Ellery Wulczyn, Narayan Hegde, Trissia Brown, Isabelle Flament-Auvigne, Fraser\\nTan, Yuannan Cai, Kunal Nagpal, Emad A Rakha, David J Dabbs, et al. Deep learning models for\\nhistologic grading of breast cancer and association with disease prognosis. NPJ breast cancer , 8(1):\\n113, 2022.\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\\nMistral 7b. arXiv preprint arXiv:2310.06825 , 2023.\\nA Johnson, T Pollard, R Mark, S Berkowitz, and S Horng. MIMIC-CXR database (version 2.0. 0).\\nPhysioNet, 2019a.\\nAlistair Johnson, Matthew Lungren, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and\\nSteven Horng. Mimic-cxr-jpg - chest radiographs with structured labels, November 2019b. URL\\nhttps://doi.org/10.13026/8360-t248 .\\nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren,\\nChih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available\\ndatabase of chest radiographs with free-text reports. Scientific data , 6(1):317, 2019c.\\nAnthony P Khawaja, Jessica N Cooke Bailey, Nicholas J Wareham, Robert A Scott, Mark Simcoe,\\nRobert P Igo Jr, Yeunjoo E Song, Robert Wojciechowski, Ching-Yu Cheng, Peng T Khaw, et al.\\nGenome-wide analyses identify 68 new loci associated with intraocular pressure and improve risk\\nprediction for primary open-angle glaucoma. Nature Genetics , 50(6):778–782, 2018.\\nAtilla P. Kiraly, Corbin A. Cunningham, Ryan Najafi, Zaid Nabulsi, Jie Yang, Charles Lau, Joseph R.\\nLedsam, Wenxing Ye, Diego Ardila, Scott M. McKinney, Rory Pilgrim, Yun Liu, Hiroaki Saito,\\nYasuteru Shimamura, Mozziyar Etemadi, David Melnick, Sunny Jansen, Greg S. Corrado, Lily\\nPeng, Daniel Tse, Shravya Shetty, Shruthi Prabhakara, David P. Naidich, Neeral Beladia, and\\nKrish Eswaran. Assistive ai in lung cancer screening: A retrospective multinational study in the\\nunited states and japan. Radiology: Artificial Intelligence , 2024. doi: 10.1148/ryai.230079. URL\\nhttps://pubs.rsna.org/doi/10.1148/ryai.230079 .\\nJonathan Krause, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi Widner, Greg S Corrado, Lily\\nPeng, and Dale R Webster. Grader variability and the importance of reference standards for\\nevaluating machine learning models for diabetic retinopathy. Ophthalmology , 125(8):1264–1272,\\n2018.\\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.\\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and\\nRichard Dufour. Biomistral: A collection of open-source pretrained large language models for\\nmedical domains. arXiv preprint arXiv:2402.10373 , 2024.\\nJeremy Lai, Faruk Ahmed, Supriya Vijay, Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni\\nAgarwal, Fayaz Jamil, Yossi Matias, Greg S Corrado, et al. Domain-specific optimization and diverse\\nevaluation of self-supervised models for histopathology. arXiv preprint arXiv:2310.13259 , 2023.\\nJason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically\\ngenerated visual questions and answers about radiology images. Scientific data , 5(1):1–10, 2018.\\n38', metadata={'source': 'data\\\\med.pdf', 'page': 37}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nChunyuanLi,HaotianLiu, LiunianLi, PengchuanZhang, JyotiAneja, JianweiYang,PingJin, Houdong\\nHu, Zicheng Liu, Yong Jae Lee, et al. Elevater: A benchmark and toolkit for evaluating language-\\naugmented visual models. Advances in Neural Information Processing Systems , 35:9287–9301,\\n2022.\\nChunyuanLi, CliffWong, ShengZhang, NaotoUsuyama, HaotianLiu, JianweiYang, TristanNaumann,\\nHoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information Processing Systems , 36, 2024.\\nPengfei Li, Gang Liu, Jinlong He, Zixu Zhao, and Shenjun Zhong. Masked vision and language pre-\\ntraining with unimodal and multimodal contrastive losses for medical visual question answering.\\nInInternational Conference on Medical Image Computing and Computer-Assisted Intervention , pages\\n374–383. Springer, 2023a.\\nPengfei Li, Gang Liu, Lin Tan, Jinying Liao, and Shenjun Zhong. Self-supervised vision-language\\npretraining for medial visual question answering. In 2023 IEEE 20th International Symposium on\\nBiomedical Imaging (ISBI) , pages 1–5. IEEE, 2023b.\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\\nbranches out , pages 74–81, 2004.\\nBo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled\\nknowledge-enhanceddatasetformedicalvisualquestionanswering. In 2021IEEE18thInternational\\nSymposium on Biomedical Imaging (ISBI) , pages 1650–1654. IEEE, 2021.\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\\ntuning. arXiv preprint arXiv:2310.03744 , 2023.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\\nneural information processing systems , 36, 2024a.\\nZhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu,\\nCheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu,\\nDinggang Shen, Xiang Li, Quanzheng Li, and Tianming Liu. Radiology-gpt: A large language model\\nfor radiology, 2024b.\\nMing Y Lu, Bowen Chen, Drew FK Williamson, Richard J Chen, Ivy Liang, Tong Ding, Guillaume\\nJaume, Igor Odintsov, Long Phi Le, Georg Gerber, et al. A visual-language foundation model for\\ncomputational pathology. Nature Medicine , pages 1–12, 2024.\\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. BioGPT:\\ngenerative pre-trained transformer for biomedical text generation and mining. Briefings in bioinfor-\\nmatics, 23(6):bbac409, 2022.\\nAnna Majkowska, Sid Mittal, David F Steiner, Joshua J Reicher, Scott Mayer McKinney, Gavin E\\nDuggan, Krish Eswaran, Po-Hsuan Cameron Chen, Yun Liu, Sreenivasa Raju Kalidindi, et al. Chest\\nradiograph interpretation with deep learning models: assessment with radiologist-adjudicated\\nreference standards and population-adjusted evaluation. Radiology , 294(2):421–431, 2020.\\nYasuhide Miura, Yuhao Zhang, Emily Bao Tsai, Curtis P Langlotz, and Dan Jurafsky. Improving\\nfactual completeness and consistency of image-to-text radiology report generation. arXiv preprint\\narXiv:2010.10042 , 2020.\\nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec,\\nEric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence.\\nNature, 616(7956):259–265, 2023a.\\n39', metadata={'source': 'data\\\\med.pdf', 'page': 38}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nMichael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka,\\nEduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner.\\nInMachine Learning for Health (ML4H) , pages 353–367. PMLR, 2023b.\\nZaidNabulsi, AndrewSellergren, ShaharJamshy, CharlesLau, EdwardSantos, AtillaPKiraly, Wenxing\\nYe, Jie Yang, Rory Pilgrim, Sahar Kazemzadeh, et al. Deep learning for distinguishing normal versus\\nabnormal chest radiographs and generalization to two unseen diseases tuberculosis and covid-19.\\nScientific reports , 11(1):15523, 2021.\\nKunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan Cameron Chen, Ellery Wulczyn, Fraser Tan, Niels\\nOlson, Jenny L Smith, Arash Mohtashamian, James H Wren, et al. Development and validation of a\\ndeep learning algorithm for improving gleason scoring of prostate cancer. NPJ digital medicine , 2\\n(1):48, 2019.\\nKunal Nagpal, Davis Foote, Fraser Tan, Yun Liu, Po-Hsuan Cameron Chen, David F Steiner, Naren\\nManoj, Niels Olson, Jenny L Smith, Arash Mohtashamian, et al. Development and validation of\\na deep learning algorithm for gleason grading of prostate cancer from biopsy specimens. JAMA\\noncology , 6(9):1372–1380, 2020.\\nEricNguyen, MichaelPoli, MarjanFaizi, ArminThomas, MichaelWornow, CallumBirch-Sykes, Stefano\\nMassaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. HyenaDNA: Long-range genomic\\nsequencemodelingatsinglenucleotideresolution. Advancesinneuralinformationprocessingsystems ,\\n36, 2024.\\nNHS. Rcr response to nhse data release on diagnostic imaging times,\\n2024. URL https://www.rcr.ac.uk/news-policy/latest-updates/\\nrcr-response-to-nhse-data-release-on-diagnostic-imaging-times/ .\\nAaron Nicolson, Jason Dowling, and Bevan Koopman. Improving chest x-ray report generation by\\nleveraging warm starting. Artificial intelligence in medicine , 144:102633, 2023.\\nNLST. National Lung Screening Trial (NLST). https://www.cancer.gov/types/lung/\\nresearch/nlst , 2014. Accessed September 15, 2021.\\nOpenAI. GPT-4V(ision) Technical Work and Authors. Technical report, OpenAI, 2023. URL https:\\n//cdn.openai.com/contributions/gpt-4v.pdf .\\nAndre GC Pacheco, Gustavo R Lima, Amanda S Salomao, Breno Krohling, Igor P Biral, Gabriel G\\nde Angelo, Fábio CR Alves Jr, José GM Esgario, Alana C Simora, Pedro BC Castro, et al. PAD-UFES-\\n20: A skin lesion dataset composed of patient data and clinical images collected from smartphones.\\nData in brief , 32:106221, 2020.\\nAnkit Pal and Malaikannan Sankarasubbu. Gemini goes to med school: Exploring the capabilities of\\nmultimodal large language models on medical challenge problems & hallucinations. arXiv preprint\\narXiv:2402.07023 , 2024.\\nPan-UKB team. https://pan.ukbb.broadinstitute.org , 2020.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for\\nComputational Linguistics , pages 311–318, 2002.\\nKendall Park, Charles Lau, Timo Kohlberger, Tom Pollard, Andrew Sellergren, Rory Sayres, and\\nAtilla P. Kiraly. MIMIC-CXR-GT Database (version 1.0.0), 2024. In preparation.\\n40', metadata={'source': 'data\\\\med.pdf', 'page': 39}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\\nDustin Tran. Image transformer. In International conference on machine learning , pages 4055–4064.\\nPMLR, 2018.\\nQizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan.\\nBiot5: Enriching cross-modal integration in biology with chemical knowledge and natural language\\nassociations. arXiv preprint arXiv:2310.07276 , 2023.\\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,\\nStefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language\\nmodels. In International Conference on Machine Learning , pages 28043–28078. PMLR, 2023.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. Journal of machine learning research , 21(140):1–67, 2020.\\nPranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J Topol. Ai in health and medicine. Nature\\nmedicine , 28(1):31–38, 2022.\\nCorentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: A benchmark and a toolkit\\nfor evaluating medical vision-language models. arXiv preprint arXiv:2402.09262 , 2024.\\nKhaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim\\nStrother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. arXiv\\npreprint arXiv:2404.18416 , 2024.\\nApaar Sadhwani, Huang-Wei Chang, Ali Behrooz, Trissia Brown, Isabelle Auvigne-Flament, Hardik\\nPatel, Robert Findlater, Vanessa Velez, Fraser Tan, Kamilla Tekiela, et al. Comparative analysis of\\nmachine learning approaches to classify tumor mutation burden in lung adenocarcinoma using\\nhistopathology images. Scientific reports , 11(1):16605, 2021.\\nJulius Sim and Chris C Wright. The kappa statistic in reliability studies: use, interpretation, and\\nsample size requirements. Physical therapy , 85(3):257–268, 2005.\\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode\\nclinical knowledge. Nature, 620(7972):172–180, 2023a.\\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\\nPfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering\\nwith large language models. arXiv preprint arXiv:2305.09617 , 2023b.\\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas\\nBeyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv\\npreprint arXiv:2106.10270 , 2021.\\nCathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey,\\nPaul Elliott, Jane Green, Martin Landray, Bette Liu, Paul Matthews, Giok Ong, Jill Pell, Alan Silman,\\nAlan Young, Tim Sprosen, Tim Peakman, and Rory Collins. UK biobank: an open access resource\\nfor identifying the causes of a wide range of complex diseases of middle and old age. PLoS Med. ,\\n12(3):e1001779, March 2015.\\nR Summers. Nih chest x-ray dataset of 14 common thorax disease categories. NIH Clinical Center:\\nBethesda, MD, USA , 2019.\\n41', metadata={'source': 'data\\\\med.pdf', 'page': 40}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nYuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Lin Sun, Zhongyi Shui, Yunlong Zhang, Honglin\\nLi, andLinYang. Pathasst: Agenerativefoundationaiassistanttowardsartificialgeneralintelligence\\nof pathology. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38 (5), pages\\n5034–5042, 2024.\\nTim Tanida, Philip Müller, Georgios Kaissis, and Daniel Rueckert. Interactive and explainable region-\\nguided radiology report generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 7433–7442, 2023.\\nRyutaro Tanno, David Barrett, Andrew Sellergren, Sumedh Ghaisas, Sumanth Dathathri, Abigail See,\\nJohannes Welbl, Karan Singhal, Shekoofeh Azizi, Tao Tu, et al. Consensus, dissensus and synergy\\nbetween clinicians and specialist foundation models in radiology report generation. arXiv preprint\\narXiv:2311.18260 , 2024.\\nOmkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad\\nAnwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs\\nsummarization using medical vision-language models. arXiv preprint arXiv:2306.07971 , 2023.\\nEkin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, and Pranav Rajpurkar. Expert-level\\ndetection of pathologies from unannotated chest x-ray images via self-supervised learning. Nature\\nBiomedical Engineering , 6(12):1399–1406, 2022.\\nAugustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G Krishnan, Barry B Rubin, and Bo Wang. Clinical\\ncamel: An open-source expert-level medical language model with dialogue-based knowledge\\nencoding. arXiv preprint arXiv:2305.12031 , 2023.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient\\nfoundation language models. arXiv preprint arXiv:2302.13971 , 2023.\\nBuu Truong, Leland E. Hull, Yunfeng Ruan, Qin Qin Huang, Whitney Hornsby, Hilary Martin, David A.\\nvan Hell, Ying Wang, Alicia R. Martin, S. Hong Lee, and Prageep Natarajan. Integrative polygenic\\nrisk score improves the prediction accuracy of complex traits and diseases. Cell Genomics , 4, 2024.\\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang,\\nAndrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical AI.\\nNEJM AI , 1(3):AIoa2300138, 2024.\\nVishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel Bibi, Samuel\\nAlbanie, and Matthias Bethge. No\" zero-shot\" without exponential data: Pretraining concept\\nfrequency determines multimodal model performance. arXiv preprint arXiv:2404.04125 , 2024.\\nTom Van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees GM Snoek, and Marcel\\nWorring. Open-ended medical visual question answering through prefix tuning of language models.\\nInInternational Conference on Medical Image Computing and Computer-Assisted Intervention , pages\\n726–736. Springer, 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\\nsystems, 30, 2017.\\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image descrip-\\ntion evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\\npages 4566–4575, 2015.\\nPeter M Visscher, William G Hill, and Naomi R Wray. Heritability in the genomics era—concepts and\\nmisconceptions. Nature Reviews Genetics , 9(4):255–266, 2008.\\n42', metadata={'source': 'data\\\\med.pdf', 'page': 41}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nFelix Vogel, Nina Shvetsova, Leonid Karlinsky, and Hilde Kuehne. Vl-taboo: An analysis of attribute-\\nbased zero-shot capabilities of vision-language models. arXiv preprint arXiv:2209.06103 , 2022.\\nXiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.\\nChestx-ray8: Hospital-scalechestx-raydatabaseandbenchmarksonweakly-supervisedclassification\\nand localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition , pages 2097–2106, 2017.\\nZhanyuWang,LingqiaoLiu,LeiWang,andLupingZhou. METransformer: Radiologyreportgeneration\\nby transformer with multiple learnable expert tokens. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 11558–11567, 2023a.\\nZhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou. R2GenGPT: Radiology report generation\\nwith frozen LLMs. arXiv preprint arXiv:2309.09812 , 2023b.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\\net al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\\ninformation processing systems , 35:24824–24837, 2022.\\nWei-HungWeng, YuannanCai, AngelaLin, FraserTan, andPo-HsuanCameronChen. Multimodalmul-\\ntitask representation learning for pathology biobank metadata prediction. CoRR, abs/1909.07846,\\n2019.\\nWei-Hung Weng, Andrew Sellergen, Atilla P Kiraly, Alexander D’Amour, Jungyeon Park, Rory Pilgrim,\\nStephen Pfohl, Charles Lau, Vivek Natarajan, Shekoofeh Azizi, et al. An intentional approach to\\nmanaging bias in general purpose embedding models. The Lancet Digital Health , 6(2):e126–e130,\\n2024.\\nMichael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth W Mahaffey, and Nigam H\\nShah. Zero-shot clinical trial patient matching with llms. arXiv preprint arXiv:2402.05125 , 2024.\\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation\\nmodel for radiology. arXiv preprint arXiv:2308.02463 , 2023.\\nEllery Wulczyn, David F Steiner, Melissa Moran, Markus Plass, Robert Reihs, Fraser Tan, Isabelle\\nFlament-Auvigne, Trissia Brown, Peter Regitnig, Po-Hsuan Cameron Chen, et al. Interpretable\\nsurvival prediction for colorectal cancer using deep learning. NPJ digital medicine , 4(1):71, 2021.\\nShawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng,\\nAttila Kiraly, Sahar Kazemzadeh, Zakkai Melamed, et al. ELIXR: Towards a general purpose x-ray\\nartificial intelligence system through alignment of large language models and radiology vision\\nencoders. arXiv preprint arXiv:2308.01317 , 2023.\\nAn Yan, Zexue He, Xing Lu, Jiang Du, Eric Chang, Amilcare Gentili, Julian McAuley, and Chun-nan\\nHsu. Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation. In Findings of\\nthe Association for Computational Linguistics: EMNLP 2021 , pages 4009–4015, 2021.\\nShen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu.\\nVideoCoCa: Video-text modeling with zero-shot transfer from contrastive captioners. arXiv preprint\\narXiv:2212.04979 , 2022.\\nFeiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser\\nUrurahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al.\\nEvaluating progress in automatic chest x-ray radiology report generation. Patterns, 4(9), 2023.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. CoCa:\\nContrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917 , 2022.\\n43', metadata={'source': 'data\\\\med.pdf', 'page': 42}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nKai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen,\\nYuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained\\ntransformer for vision, language, and multimodal tasks. arXiv preprint arXiv:2305.17100 , 2023a.\\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi\\nXie. PMC-VQA: Visual instruction tuning for medical visual question answering. arXiv preprint\\narXiv:2305.10415 , 2023b.\\nYuchen Zhou, Justin T Cosentino, Taedong Yun, Mahantesh I Biradar, Jacqueline Shreibati, Dongbing\\nLai, Tae-Hwi Schwantes-An, Robert Luben, Zachary R McCaw, Jorgen Engmann, et al. Utilizing\\nmultimodal ai to improve genetic analyses of cardiovascular traits. medRxiv , pages 2024–03, 2024.\\nLe Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, and\\nWentao Zhang. Protllm: An interleaved protein-language llm with protein-as-word pre-training.\\narXiv preprint arXiv:2403.07920 , 2024.\\n44', metadata={'source': 'data\\\\med.pdf', 'page': 43}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nAppendix\\nA.1. Additional data details\\nA.1.1. Revised MIMIC-CXR classification labels\\nOne of the limitation of the MIMIC-CXR dataset is the lack of ground-truth labels. MIMIC-CXR JPG\\n(Johnson et al., 2019b) extracted structured labels from 277,827 radiology reports using CheX-\\npert (Irvin et al., 2019), a natural language processing (NLP) tool to extract observations from\\nradiology reports. In order to improve upon these labels on the test subset, we utilized Med-PaLM\\n2 (Singhal et al., 2023b) coupled with US-based board certified radiologists to refine those labels.\\nThis work is further adjudication of the labels used in Xu et al. (2023). We first used a keyword search\\nto identify reports containing text associated with the a given finding ( e.g.,“Cardiomegaly”). Next,\\nMed-PaLM 2 was applied to the flagged radiology reports on a per-label basis using two queries shown\\nin Table A.1 for a total of 23,824 queries. All identified positive and negative labels that disagreed\\nwith the original labels were flagged for human verification.\\nThree US-based board certified radiologists reviewed the 1,378 flagged labels and a fourth\\nacademic US-based board certified thoracic radiologist adjudicated the responses of reviewer dis-\\nagreements. For each finding and report, radiologists selected one of four possible labels defined\\nby MIMIC-CXR JPG: positive, negative, uncertain, and not mentioned. Zero-round adjudication\\nwas performed on the reviewers’ annotations. There was strong inter-rater agreement (Fleiss’ 𝜅=\\n0.71); reviewers were unanimous for 77% of the labels. In cases of disagreement between reviewers,\\nmajority vote was used (21%), and when all three reviewers disagreed (2%) a senior academic\\nthoracic radiologist provided the final determination.\\nIn the final analysis of the flagged reports and findings, Med-PaLM 2’s label matched the ground\\ntruth 66% of the time while the original labels were correct in 19% of the cases. The labels are in\\npreparation to be released (Park et al., 2024).\\nTable A.1|Prompt styles used for Med-Palm2 in extracting structured labels. Two prompt styles were\\nused to extract labels from the given MIMIC-CXR radiology reports. The following shows the template used for\\nidentifying fractures.\\nPrompt Style Prompt Example\\nBot[Bot] I’m a helpful radiology assistant, who provides concise answers\\nto questions about information in a chest x-ray report.\\n[User] Determine the answer to the following question:\\n[Does the patient have a fracture?],\\ngiven the context of the follow chest x-ray report: <REPORT TEXT>\\nDo not mention conditions or parts of the report not relevant to the question.\\nMake sure to only answer: [Does the patient have a fracture?]\\n[Bot]\\nQuestionYou are a helpful medical knowledge assistant.\\nProvide useful, complete, concise, and scientifically-grounded\\nqueries to radiology reports.\\nDoes this report mention that the patient has a fracture?\\nReport:<REPORT TEXT>\\n45', metadata={'source': 'data\\\\med.pdf', 'page': 44}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nA.1.2. Prompts for VQA and CXR classification evaluations\\nWeexploredbothbinaryquestionpromptingforeachoftheevaluatedtop5conditionsforMIMIC-CXR\\nincluding the abnormal/normal class, as well as multi-select prompts for all of them at once on the\\nvalidation set. Since binary prompts overall yielded better macro F1 scores for Med-Gemini, these\\nwere used at evaluation. For Gemini Ultra binary question were used for the top-5 conditions and\\na multi-select prompt for the normal/abnormal condition, see Tables A.3 and A.4. Each prompt\\ntemplate was crafted for each model in aiming to optimize its performance, which yielded very short\\nprompt templates for both classification and VQA for Med-Gemini, since it was fine-tuned with clinical\\nquestions.\\nTable A.2|Zero-shot prompt templates used for VQA evaluations (excluding formatting tokens). The\\nfollowing prompt templates were used for each model and VQA evaluation data set. The < question >\\nplaceholder was replaced with the individual question of each VQA triplet.\\nVQA Dataset Model Prompt Template\\nVQA-RadGemini< image > You are a helpful radiology assistant. Given this radiology\\nimage, which can be frontal chest X-ray provide a very short, concise\\nanswer, like: “pleural effusion,” “yes,” “right,” to the following question:\\n< question >\\nMed-Gemini < image >< question >\\nSlake-VQAGemini< image > You are a helpful radiology assistant. Given this radiology\\nimage, which can be a frontal chest X-ray, a single slice head or abdominal\\nCT or MR image, provide a very short, concise answer, like: “pleural\\neffusion,” “yes,” “right,” to the following question: < question >\\nMed-Gemini < image >< question >\\nPathVQAGemini< image > You are a helpful pathology assistant. Given the pathology\\nimage, provide a concise answer, like: “pleural effusion,” “yes,” “right,” to\\nthe following question: < question >\\nMed-Gemini < image >< question >\\nA.1.3. New balanced splits for VQA-Rad dataset\\nThe official train/test split of the VQA-Rad (Lau et al., 2018) dataset comprises 1,797 QA pairs for\\ntraining (i.e. dataset field QID_para ∈{’freeform’, ’para’}) for 313 different MedPIX ®images (per\\nfield IMAGEID), and 451 QA pairs for 203 different images in the test set (i.e. field QID_para ∈\\n{’test_freeform’, ’test_para’}). Although images were sampled such that each is not only for a different\\ncase, but also a different patient, see (Lau et al., 2018), 202 of the test IMAGEIDs and also match the\\ntrain set IMAGEIDs. Hence most of the test images also appear in the train set, only the questions\\nand answers differ. For some VQAs even the latter is not completely true, since VQA-Rad contains\\nparaphrased questions which share the same answer.\\nTo remove this train/test contamination issue, in Xu et al. (2023) we proposed a different\\nvalidation/test split, which is based on the IMAGEIDs in order to ensure disjoint images and thus\\npatients. In this work we split this relatively large test set further into a new test and train set, which\\nare roughly equal-sized, and assign a few remaining former test set IMAGEIDs and corresponding\\nVQAs to the existing validation set, such that all three new splits not only are roughly equal-sized,\\nbut their ratio of open to closed questions (as determined by field A_TYPE) are approximately equal\\nwithin each of the three depicted anatomical regions (chest, head and abdomen), see Table A.5. We\\nchose to equalize this ratio since open-ended questions are more difficult for AI models, and the level\\nof difficulty ought to be similar for each new split. Similarly, while swapping individual IMAGEIDs\\n46', metadata={'source': 'data\\\\med.pdf', 'page': 45}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable A.3|Zero-shot prompts used evaluating classification performance on chest X-ray. Binary questions\\nwere employed for Med-Gemini on all CXR classification conditions, and for Gemini Ultra on all except for the\\nnormal/abnormal condition (label “No Finding”). The < question > placeholder was replaced with the label-\\ndependent text listed in Table A.4, while < view position > with the respective MIMIC-CXR meta information\\n(e.g. “AP”).\\nModel Used for Prompt template\\nGemini Top-5 Conditions< image > You are a helpful radiology assistant.\\nThe following is a question about findings in chest X-ray in the frontal\\nview. Answer only with Yes or No.\\nQ: Given the X-ray < view position > image, < question > indicated\\nin this chest X-ray image?\\nA:\\nMed-GeminiTop-5 Conditions\\nand\\nNormal/Abnormal< image >< question >\\nGemini Normal/Abnormal< image > You are a helpful radiology assistant.\\nThe following are multiple-choice questions about findings in chest\\nX-ray in the frontal view. Identify if a specific type of abnormality\\nis shown in the X-ray by responding with the corresponding answer\\nchoice letter(s).\\nQ: Given the X-ray image, which of the following abnormalities are\\nindicated by the image?\\n(A) Atelectasis (B) Cardiomegaly (C) Consolidation (D) Edema (I)\\nPleural Effusion (O) No Abnormality (E) Enlarged Cardiomediastinum\\n(F) Fracture (G) Lung Lesion (H) Lung Opacity (I) Pleural Effusion (J)\\nPleural Other (K) Pneumonia (L) Pneumothorax (M) Support Devices\\nand associated question and answers, we approximately equalized the distribution of question types\\n(field Q_TYPE) within each split, in order to gain similar ones, see Table A.6.\\nA.1.4. Polygenic risk prediction\\nWe crafted prompts for predicting the status of various health outcomes using both an individual’s\\nPRS image and their demographic information. An example prompt for predicting coronary artery\\ndisease is shown in Table A.9.\\nForlinearprobesofout-of-distributionoutcomes, weuseddataofarelatedin-distributionoutcome\\nto train the linear probe and then evaluated the predictions on the out-of-distribution outcome. For\\nexample, in order to evaluate diabetic retinopathy, we train the linear probe to predict type 2 diabetes\\nand evaluate the type 2 diabetes predictions on diabetic retinopathy data. In general, the most related\\nin-distribution outcome is defined as the outcome with the highest Matthew’s correlation coefficient\\nwith the out-of-distribution outcome across individuals in our training set (Table A.11). To evaluate\\nMed-Gemini-Polygenic, we directly prompted Med-Gemini-Polygenic to predict the out-of-distribution\\noutcome without providing any information about correlations between out-of-distribution and\\nin-distribution outcomes.\\n47', metadata={'source': 'data\\\\med.pdf', 'page': 46}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable A.4|Question arguments used for binary classification prompt templates. We used the following\\narguments for the < question > placeholder in the binary classification prompts, Table A.3, which were\\ntriggered depending on the corresponding CheXpert label to be in {0.0,1.0}.\\nCondition Argument for Med-Gemini prompt Argument for Gemini prompt\\nAtelectasis Is there atelectasis? is atelectasis\\nCardiomegaly Is there cardiomegaly? is cardiomegaly\\nConsolidation Is there consolidation? is consolidation\\nEdema Is there pulmonary edema? is pulmonary edema\\nPleural Effusion Is there pleural effusion? is pleural effusion\\nTable A.5|Distribution of VQA-Rad answer types (closed- vs. open-ended) across the new balanced\\nsplits for each anatomical region. The new train/validation/test splits not only guarantee the images and\\nthus patients to be disjoint, but also provide a similar ratio of open to close-ended questions (dataset field\\nA_TYPE) per anatomical region, which typically affects VQA performance of AI models due to open-ended\\nquestions being more difficult to answer correctly.\\nAnatomical Region Split (balanced)Number of question-answers pairs Ratio of open to closed\\nOpenClosed Total\\nAbdomenTrain 104 153 257 68.0%\\nValidation 97 143 240 67.8%\\nTest 99 143 242 69.2%\\nChestTrain 94 185 279 50.8%\\nValidation 107 161 268 66.5%\\nTest 82 165 247 49.7%\\nHeadTrain 131 118 249 111.0%\\nValidation 113 107 220 105.6%\\nTest 122 144 246 84.7%\\nAll 949 1,299 2,248 73.1%\\nTableA.6|DistributionofVQA-Radquestiontypesacrossthenewbalancedsplits. Inadditiontocomparable\\nopen-to-closed-question ratios, the question types (dataset field Q_TYPE) distribute similarly as well.\\nAnatomical Region Abdomen Chest HeadAllSplit (balanced) Train Val. Test Train Val. Test Train Val. Test\\nTotal 257 240 242 279 268 247 249 220 246 2248\\nPresence 105 95 100 101 92 93 79 67 68 800\\nPositional 26 27 21 37 36 42 35 42 50 316\\nAbnormality 19 18 27 15 40 21 22 25 15 202\\nOther 20 20 22 26 19 19 22 14 32 194\\nModality 25 13 23 19 17 13 35 24 16 185\\nSize 17 20 11 41 26 27 10 5 14 171\\nPlane 12 9 13 14 12 13 17 16 14 120\\nAttribute Other 13 19 7 5 9 8 4 12 10 87\\nOrgan 5 4 2 10 8 7 13 3 7 59\\nColor 6 7 8 2 0 0 9 7 13 52\\nCounting 3 8 3 5 1 2 0 1 1 24\\n48', metadata={'source': 'data\\\\med.pdf', 'page': 47}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable A.7|Examples of some of our curated captions for histopathology patches used in our training set.\\nTask Curated Captions\\nBreastcancerdetectionin\\nlymph nodesMicroscopic view of a lymph node with infiltrating malignant epithelial cells consistent\\nwith metastatic breast carcinoma.\\nLymph node section on histopathology demonstrating features of metastatic carcinoma,\\nlikely originating from the breast.\\nRegion of a lymph node on histopathology, showing predominantly lymphocytes and\\nimmune cells.\\nHistologic subtyping for\\nlung adenocarcinomaH&E histopathology image demonstrating acinar subtype lung adenocarcinoma with\\nglandular formation.\\nCribriform pattern in lung adenocarcinoma with closely packed, back-to-back glands\\nlacking an organized pattern.\\nMicroscopic view of lung adenocarcinoma showing tumor cells spreading along the\\npreexisting alveolar architecture, consistent with the lepidic subtype.\\nProstate Gleason gradingMicroscopic view of a prostate biopsy with discrete, uniformly sized and shaped glands\\nconsistent with Gleason pattern 3 prostate cancer.\\nProstate biopsy image highlighting areas of fused glands or poorly formed glands, consis-\\ntent with Gleason pattern 4 carcinoma.\\nMicroscopic view of a prostate tissue with discrete, uniformly sized and shaped glands\\nconsistent with Gleason pattern 3 prostate cancer.\\nBreast cancer nuclear\\npleomorphism gradingMicroscopicviewofinvasivebreastcarcinomashowingmoderatenuclearatypia,including\\nenlarged nuclei and prominent nucleoli (nucleopleomorphism score 2).\\nFocus on infiltrating tumor cells within an H&E stained image, demonstrating bland\\nnuclear features suggestive of low-grade invasive breast carcinoma.\\nH&E histopathology image demonstrating invasive breast carcinoma with high-grade\\nnuclear features (nucleopleomorphism score 3).\\nBreast cancer tubule for-\\nmation gradingThis microscopic view demonstrates a predominance of well-defined glandular structures,\\nindicating a tubule formation score of 1 in this breast carcinoma.\\nThe presence of some discernible tubules, alongside regions of less-defined glandular\\narchitecture, indicates a tubule formation score of 2.\\nA tubule formation score of 3 is evident within this H&E stained image, where the invasive\\ncarcinoma shows a scarcity of well-defined glandular structures.\\nCervical dysplasia grad-\\ningH&E stained image of a cervical biopsy demonstrating features of cervical intrapethileal\\nneoplasia grade 1 (CIN 1), including nuclear atypia in the lower third of the epithelium.\\nMicroscopic view of a cervical biopsy with CIN 1, showing enlarged nuclei and increased\\nnuclear-to-cytoplasmic ratio in the basal layer of the epithelium.\\nH&E histopathology of a cervical biopsy focusing on high-grade dysplasia, showing\\nprominent nuclear abnormalities and disruption of the normal epithelial architecture.\\n49', metadata={'source': 'data\\\\med.pdf', 'page': 48}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable A.8|Overview of patch-level histopathology datasets used for fine-tuning and linear-probe evaluation.\\nTasks adopted from (Lai et al., 2023). The OOD column indicates whether the task was included in Med-Gemini\\nfine-tuning. Number of slides shows the counts split across train, validation, and test sets, respectively.\\nDataset Task Number of slides Classes OOD\\nCAMELYON16\\n(Bejnordi et al., 2017)Breast cancer detection\\nin lymph nodes216/54/258 Tumor, Non-Tumor -\\nLung AD\\n(Sadhwani et al., 2021)Histologic subtyping\\nfor lung adenocarcinoma73/25/50 9 classes∗-\\nGleason NCB\\n(Nagpal et al., 2020)Gleason grading on prostate\\nneedle core biopsies178/85/88 Benign, GP3, GP4, GP5 -\\nGleason RP\\n(Nagpal et al., 2019)Gleason grading on radical\\nprostatectomies550/259/202 Benign, GP3, GP4, GP5 -\\nBreast IC\\n(Jaroensri et al., 2022)Breast invasive\\ncarcinoma detection573/288/669Benign, DCIS,\\nInvasive Carcinoma-\\nBreast NP\\n(Jaroensri et al., 2022)Breast cancer nuclear\\npleomorphism grading681/343/945 NP1, NP2, NP3 -\\nBreast TF\\n(Jaroensri et al., 2022)Breast cancer tubule\\nformation grading681/343/945 TF1, TF2, TF3 -\\nCIN\\n(Lai et al., 2023)Cervical dysplasia\\ngrading329/74/229 Non-tumor, CIN 1, CIN 2+ -\\nCRC\\n(Wulczyn et al., 2021)Colorectal carcinoma\\ndetection149/51/44 Tumor, Non-Tumor -\\nTissue type\\n(Weng et al., 2019)Tissue type classification\\n(internal dataset)17319/6488/6719 16 tissue types†✓\\nTCGA study\\n(Lai et al., 2023)TCGA study type\\nclassification2952/1466/1489 10 TCGA study types‡✓\\n∗Lung AD histologic subtypes and other classes: Acinar, Cribriform, Lepidic, Micropapillary, Papillary, Solid, Leukocyte, Necrosis,\\nNon-tumor.\\n†Tissue types: Appendix, Breast, Cervix, Colon and rectum, Fallopian Tube, Gallbladder, Liver, Lymph node, Ovary, Placenta, Prostate,\\nSkin, Thyroid, Upper GI, Uterus, Vas deferens.\\n‡TCGA study types: BLCA, BRCA, COAD, HNSC, KIRC, LIHC, LUAD, LUSC, OV, STAD.\\nTable A.9|Examples of a prediction prompt for coronary artery disease using an individual’s PRS image\\nand demographic information. For privacy reasons, the PRS image shown is an average PRS image over 100\\nindividuals.\\nImage Prompt Target\\n<img>Giventhisgenomicdata, and\\nthe following health information:\\nage: 60;\\nsex: female;\\nbody mass index: 26.1;\\npredict whether the individual has\\nthe following condition or not. Re-\\nspond “Yes” or “No”.\\nCoronary artery disease:No\\n50', metadata={'source': 'data\\\\med.pdf', 'page': 49}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable A.10|Overview of UK Biobank fields used to compile health outcomes for training and evaluating\\nMed-Gemini-Polygenic.\\nHealth outcome UK Biobank fields involved Conversion process\\nCoronary artery disease 131307If source of report of I25 (chronic ischaemic heart\\ndisease) available (field 131307).\\nStroke 6150, 131368, 131369,\\n41271, 41270Logical OR of:\\nchoice 3 of touchscreen questionnaire (field 6150),\\nfirst reported stroke (field 131368),\\nsource of report stroke (field 131369),\\nICD-9 code 434.91 in field 41271,\\nICD-10 codes I63.* and I64.* in field 41270.\\nType 2 Diabetes 20002, 41270Code 1223 in self-reported non-cancer illness code\\n(field 20002), or ICD-10 codes E11.* in field 41270.\\nGlaucoma 4689, 5326, 5327, 6148,\\n20003, 41202, 41203, 41205,\\n42104See Khawaja et al. (2018) for details.\\nCOPD 6152, 41270, 41271, 42040 See Cosentino et al. (2023) for details.\\nRheumatoid arthritis 131851 If source of report of M06 (field 131851).\\nMajor depression 20126, 41270Code 3, 4, 5 in bipolar and major depression status\\n(field 20126), or ICD-10 codes F32.* and F33.* in field\\n41270.\\nAll-cause mortality 40000 If there is a value in date of death (field 40000).\\nHypertension 20002, 41271, 41270Logical OR of:\\nCode 1065 in self-reported non-cancer illness code\\n(field 20002),\\nICD-9 code 401.* and 405.* in field 41271,\\nICD-10 codes I10 and I15.* in field 41270.\\nHypercholesterolemia 30780, 30760Labels are calculated using LDL (30780) and\\nHDL(30760) by:\\nLDL cholesterol >= 190 mg/dL OR\\n(LDL >= 160 mg/dL AND\\n((HDL < 40 mg/dL AND person is Male) OR\\n(HDL < 55 mg/dL AND person is Female)))\\nData reported in units of mmol/L,\\nto convert to mg/dL, multiply these by 38.67.\\nAtrial Fibrillation 20002, 41271, 41270Logical OR of:\\nCode 1471 in self-reported non-cancer illness code\\n(field 20002),\\nICD-9 code 4273 in field 41271,\\nICD-10 codes I48(|0|1|2|9) in field 41270.\\nDiabetic Retinopathy 6148, 20002, 5890, 5901,\\n41271, 41270Logical OR of:\\nCode 1276 in self-reported non-cancer illness code\\n(field 20002),\\nCode 1 in eye problem/disorder (6148),\\nCode 1, 2, 3 in Which eye(s) affected by\\ndiabetes-related eye disease(5890),\\nThere is a value in Age when diabetes-related eye\\ndisease diagnosed (5901),\\nICD-9 code 3620 in field 41271,\\nICD-10 codes H360 in field 41270.\\nAsthma 20002, 41270Code 1111 in self-reported non-cancer illness code\\n(field 20002), or ICD-10 codes J45.*, J46.* in field 41270.\\nPneumonia 41271, 41270ICD-9 code 401.* and 480, 481, 482, 483, 484,\\n486 in field 41271,\\nICD-10 codes J12.*, J13, J14, J15.*, J16.*, J17.*,\\nJ18.* in field 41270.\\n51', metadata={'source': 'data\\\\med.pdf', 'page': 50}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable A.11|Polygenic risk prediction health outcomes correlation. Most correlated In-distribution (ID)\\nhealth outcomes for each out-of-distribution (OOD) health outcome. In general, the most related in-distribution\\noutcomeisdefinedastheoutcomewiththehighestMatthew’scorrelationcoefficientwiththeout-of-distribution\\noutcome across individuals in our training set.\\nOOD health outcome ID health outcome Matthew’s correlation coefficient\\nHypertension Coronary Artery Disease 0.266\\nHypercholesterolemia Major Depression 0.007\\nAtrial Fibrillation Coronary Artery Disease 0.240\\nDiabetic Retinopathy Type 2 Diabetes 0.325\\nAsthma COPD 0.186\\nPneumonia All-cause Mortality 0.352\\n52', metadata={'source': 'data\\\\med.pdf', 'page': 51}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nA.2. Additional results\\nA.2.1. Data-efficient classification\\nWeperformeddata-efficientclassificationforChestX-rayclassificationtaskfocusingonexamplesacross\\n8differentfindings(atelectasis, cardiomegaly, airspaceopacity, consolidation, fracture, pneumothorax,\\npleural effusion, and pulmonary edema). We also deploy two out-of-distribution datasets including\\nChestX-ray14 and CheXpert for this purpose. Our data-efficient classification follows the protocol\\nfrom (Xu et al., 2023) except that instead of training a Multilayer Perceptron (MLP) as a nonlinear\\nclassifier, we train a linear probe on top of the frozen image encoder. Following the ELEVATER(Li\\net al., 2022) method, we initialize the weights of the final linear layer with the text embeddings for\\nthe class label. Training parameters includes a learning rate of 0.2, a batch size of 512, and 300\\nepochs utilizing the Layer-wise Adaptive Rate Scaling (LARS) optimizer.\\nIn alignment with previous best-in-class method, ELIXR (Xu et al., 2023), the linear classifiers\\nwere trained on 5 different varying sample sizes including 0.01% to 100% subsets of the training\\ndata to facilitate direct comparability of results to Xu et al. (2023). The smallest sample size includes\\n64 samples. Figure A.1 shows aggregated results of Med-Gemini vs.ELIXR on ChestX-ray14 and\\nCheXpert (Xu et al., 2023) for 5 and 6 various runs, respectively. Comparison between data-efficient\\nclassification results of Med-Gemini vs.ELIXR reveals that linear probes trained on top of visual\\nembeddings from Med-Gemini exhibit robust performance in data-efficient classification, although\\napproximately one order of magnitude inferior than ELIXR at the sample size as low as 64 samples.\\n82838485\\nTraining set sample size (# images)5060708090100AUC\\nELIXR\\nMed-Gemini\\nFigure A.1|Data-efficient classification results for Med-Gemini vs.ELIXR. We target classification across 8\\ndifferent findings (atelectasis, cardiomegaly, airspace opacity, consolidation, fracture, pneumothorax, effusion,\\nand pulmonary edema) and 2 out-of-distribution datasets (ChestX-ray14 and CheXpert). Linear probes trained\\non top of visual embeddings from Med-Gemini show strong performance on classification although roughly one\\norder of magnitude inferior to ELIXR at sample sizes as low as 64 examples.\\nA.2.2. Polygenic risk prediction\\nBeyond evaluating Med-Gemini-Polygenic, we also compared linear probes of the Med-Gemini-\\nPolygenic embeddings to linear probes of the demographics only and the ensemble of PRSs and\\ndemographics. Figure A.2 uses the same balanced sets of 400 individuals as used in Figure 5, and\\nFigure A.3 uses larger balanced sets containing all the positive cases per health outcome and an\\nequal number of controls. The AUC metrics are relatively consistent between both evaluation sets.\\nFurthermore, we computed Med-Gemini-Polygenic performance on coronary artery disease and COPD\\nin 4000-sample evaluations (Figure A.4), and observed stable results. Taken together, these results\\nsuggesting that our evaluation set of 400 individuals is representative of overall model performance.\\n53', metadata={'source': 'data\\\\med.pdf', 'page': 52}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nStrokeMajor\\nDepressionRheumatoid\\n      ArthritisGlaucoma All-cause\\n     MortalityCoronary\\nArtery DiseaseType\\n2 DiabetesCOPD405060708090AUCIn-distribution\\nHyperchol-\\nesterolemiaAsthma Hyper-\\ntension Pneumonia Atrial\\n          FibrillationDiabetic\\n     Retinopathy405060708090AUCOut-of-distribution\\nDemographics only Ensemble of PRSs & demographics Embeddings & demographics\\nFigure A.2|Health outcome prediction using linear probes for both in-distribution (ID) and out-of-distribution\\n(OOD)outcomesonbalancedevaluationsetsof400individuals. “Demographicsonly”usedalinearprobeof age,\\nsex, and BMIto predict each health outcome, “Ensemble of PRSs and demographics” combined demographics\\nwithall7,145PRSsinalinearprobe,and“Embeddingsanddemographics”combineddemographicswithgenetic\\nrisk embeddings. For OOD health outcomes, the linear probes were trained to predict the most-correlated ID\\noutcome (Table A.11), and those predictions were then evaluated on the OOD outcome.\\nIn addition, we demonstrated that using the Med-Gemini-Polygenic framework likely results in\\nbetterpredictiveperformancethanlinearmodelstrainedwithallPRSfeaturizationsplusdemographics\\nregardlessoffuturesamplesizesavailablebyconductingsamplesizeablationtestsonthePRSensemble\\nmodels. We observed performance plateaus for the linear model with at most 104samples (Figure\\nA.6).\\nFinally, we investigated the relative contributions of the genomic embedding and modeling non-\\nlinear interactions between genomic representations and demographic information by comparing\\nthe performance of Med-Gemini-Polygenic to two other non-linear models: a gradient-boosted\\ndecision tree (GBDT) of the “Embeddings and demographics” (“Embeddings”) and a GBDT of the\\nmost correlated individual PRS at each of the three significance thresholds and demographics (“Best\\nPRSs”). Med-Gemini-Polygenic and the GBDT of “Embeddings and demographics” yield comparable\\nperformance across all traits, and consistently outperform the GBDT of “Best PRS” for in-distribution\\noutcomes, confirmingtheimportanceofbothmulti-PRSpredictorsandaccuratelymodelingnon-linear\\ninteractions between genetic contributors and demographic information (Table A.12).\\n54', metadata={'source': 'data\\\\med.pdf', 'page': 53}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nStrokeMajor\\nDepressionGlaucomaRheumatoid\\n      ArthritisAll-cause\\n     MortalityCOPD\\nCoronary\\nArtery DiseaseType\\n2 Diabetes405060708090AUCIn-distribution\\nHyperchol-\\nesterolemiaAsthma Hyper-\\ntension Atrial\\n          FibrillationPneumonia Diabetic\\n     Retinopathy405060708090AUCOut-of-distribution\\nDemographics only Ensemble of PRSs & demographics Embeddings & demographics\\nFigure A.3|Health outcome prediction using linear probes for both in-distribution (ID) and out-of-distribution\\n(OOD) outcomes on larger balanced evaluation sets. For each outcome, the evaluation set includes all positive\\ncases in our test set and the same number of negative controls. “Demographics only” used a linear probe of age,\\nsex, and BMIto predict each health outcome, “Ensemble of PRSs and demographics” combined demographics\\nwithall7,145PRSsinalinearprobe,and“Embeddingsanddemographics”combineddemographicswithgenetic\\nrisk embeddings. For OOD health outcomes, the linear probes were trained to predict the most-correlated ID\\noutcome (Table A.11), and those predictions were then evaluated on the OOD outcome.\\nCoronary\\nArtery DiseaseCOPD405060708090AUC400 samples\\nCoronary\\nArtery DiseaseCOPD405060708090AUC4,000 samples\\nDemographics only\\nEnsemble of PRSs & Demographics\\nMed-Gemini (PRSs & Demographics)\\nFigure A.4|Health outcome prediction performance on evaluation sets of 400 and 4,000 balanced samples for\\ncoronary artery disease and COPD. The left plot replicates Figure 5 results. The right plot is the analogous\\nperformance in larger samples of 4,000 balanced case-control datasets.\\n55', metadata={'source': 'data\\\\med.pdf', 'page': 54}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nMajor\\nDepressionStrokeGlaucomaRheumatoid\\n      ArthritisAll-cause\\n     MortalityCoronary\\nArtery DiseaseCOPDType\\n2 Diabetes405060708090AUCIn-distribution\\nHyperchol-\\nesterolemiaAsthmaPneumoniaHyper-\\ntension Atrial\\n          FibrillationDiabetic\\n     Retinopathy405060708090AUCOut-of-distribution\\nDemographics only Med-Gemini (Demographics) Embeddings & Demographics Med-Gemini (PRSs & Demographics)\\nFigure A.5|Health outcome prediction using linear probes and Med-Gemini-Polygenic for both in-distribution\\n(ID) and out-of-distribution (OOD) outcomes on balanced evaluation sets of 400 individuals. “Demographics\\nonly” used a linear probe of age,sex, and BMIto predict each health outcome, and “Embeddings and\\ndemographics” combined demographics with genetic risk embeddings. Med-Gemini-Polygenic was prompted\\nwith either demographic information only or the individual’s PRS image and demographic information. For\\nOOD health outcomes, the linear probes (“Demographics only” and “Embeddings and demographics”) were\\ntrained to predict the most-correlated ID outcome (Table A.11), and those predictions were then evaluated on\\nthe OOD outcome.\\n56', metadata={'source': 'data\\\\med.pdf', 'page': 55}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nIn-distribution Data-efficient Prediction Performance\\nOut-of-distribution Data-efficient Prediction Performance\\n102103104\\nHypercholesterolemia5060708090AUROC\\n102103104\\nAsthma\\n102103104\\nHypertension\\n102103104\\nPneumonia\\n102103104\\nAtrial fibrillation\\n102103104\\nDiabetic retinopathy\\n102103104\\nStroke5060708090AUC\\n102103104\\nMajor depression\\n102103104\\nRheumatoid arthritis\\n102103104\\nGlaucoma\\n102103104\\n102103104\\n102103104\\n102103104\\nCOPD\\n Med-Gemini (PRSs & Demographics) Med-Gemini (Demographics) All-cause mortality   Coronary artery disease   Type 2 diabetes\\nEnsemble of PRSs & Demographics on Full Training Ensemble of PRSs & Demographics\\nFigure A.6|Comparing health outcomes prediction performance of Med-Gemini-Polygenic (zero-shot for\\nout-of-distribution outcomes) with PRS plus demographics linear probes trained with different sample sizes.\\n“Ensemble of PRSs & Demographics” are linear models trained against the given health outcome (or the most\\nrelated ID health outcome for OOD outcomes) using all 7,145 PRSs and demographics. Dashed constant lines\\nshow the performance of Med-Gemini-Polygenic on predicting outcomes, given only demographics or genetic\\nrisk “image” plus demographics. All linear models in this experiment were trained on population-prevalence\\ndata splits.\\nTable A.12|Prediction performance measured by AUC for non-linear models of genomics and demo-\\ngraphics. The strong performance of Med-Gemini-Polygenic stems from both the inclusion of multiple PRS in\\nthe genomic representation and capturing the non-linear interactions between genomics and demographics.\\n“Embeddings”, a GBDT of the Med-Gemini embeddings and demographics. “Best PRS”, a GBDT of the most\\ncorrelated individual PRS with the outcome at each significance level and demographics.\\nOutcome Med-Gemini-Polygenic Embeddings Best PRS\\nMajor Depression 70.2 68.3 57.1\\nStroke 78.1 77.3 75.0\\nGlaucoma 78.9 78.1 80.4\\nRheumatoid Arthritis 79.2 79.0 76.4\\nAll-cause Mortality 79.9 80.3 75.3\\nCoronary Artery Disease 82.5 79.7 77.3\\nCOPD 83.9 83.9 72.7\\nType 2 Diabetes 86.5 86.1 85.3\\nHypercholesterolemia 52.4 54.1 53.5\\nAsthma 57.7 59.0 63.9\\nPneumonia 71.6 73.3 70.3\\nHypertension 72.0 70.2 72.1\\nAtrial Fibrillation 78.7 78.4 68.5\\nDiabetic Retinopathy 87.0 89.0 84.5\\n57', metadata={'source': 'data\\\\med.pdf', 'page': 56}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nA.2.3. MIMIC-CXR classification\\nTable A.13 shows the comparison between performance of Med-Gemini and Gemini Ultra measure by\\nF1-score for the original label and revised label as explained in Section A.1.1. Revised MIMIC-CXR\\nlabels significantly improve chest X-ray classification performance measured by F1-score (%). Our\\nresults demonstrate the impact of accurate ground truth on model evaluation.\\nTable A.13|MIMIC-CXR classification performance using the revised labels vs.the original. Performance\\non chest X-ray MIMIC-CXR classification measured by F1-score (%) when using the original vs.the revised\\nlabels. Our experiment shows the measured performance is improved when the revised version of labels are\\nused as the ground-truth.\\nDatasets ConditionOriginal Labels - F1(%) Revised Labels - F1(%)\\nMed-Gemini Gemini Ultra Med-Gemini Gemini Ultra\\nMIMIC-CXRAtelectasis 99.3 90.7 99.8 88.1\\nCardiomegaly 92.8 93.8 94.1 94.6\\nConsolidation 79.5 76.3 82.0 77.0\\nEdema 86.7 86.1 86.8 86.4\\nPleural Effusion 91.0 89.3 90.8 88.1\\nClass-weighted average 92.1 89.0 91.2 88.6\\nNormal/Abnormal 40.9 6.3 42.0 29.7\\nA.2.4. Histopathology classification\\nTableA.14detailsthelinearprobingresultsforthehistopathologypatch-classificationtask,reporting1-\\nvs-rest AUC (%) with 95% confidence intervals. The confidence intervals were obtained using blocked\\nbootstrap resampling over test set slides with 10,000 replicates. Our model’s image embeddings\\nmatchtheperformanceofthehistopathology-specializedmodel(PathSSL)on6outof9in-distribution\\ntasks, with room for improvement on the remaining tasks. While Gemini and Med-Gemini-2D perform\\nsimilarly overall, Med-Gemini shows a trend towards higher mean AUC on most in-distribution tasks\\nand both out-of-distribution tasks.\\n58', metadata={'source': 'data\\\\med.pdf', 'page': 57}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nTable A.14|Histopathology patch-classification with linear-probe. We measure the 1- 𝑣𝑠-rest AUCs (%) from\\nlinear-probing on our histopathology patch-classification tasks. The 95% confidence intervals were obtained\\nby a blocked bootstrap resampling (over test set slides) using 10000 replicates. The image embeddings from\\nMed-Gemini perform on par with a histopathology-specialized foundation model (PathSSL) on 6 of the 9\\nin-distributiontasks. Theremaining3in-distributionand2out-of-distributiontasksleaveroomforimprovement.\\nGemini performs similarly to Med-Gemini-2D across all tasks, although Med-Gemini trends higher in terms of\\nmean AUC(%) on 7 of the 9 in-distribution tasks, and both out-of-distribution tasks.\\nDataset AugReg-ImageNet Gemini Ultra Med-Gemini PathSSL (Lai et al., 2023)\\nCAMELYON1695.09\\n(93.38 - 96.58)96.76\\n(95.13 - 98.13)98.48\\n(97.63 - 99.17)99.00\\n(98.26 - 99.53)\\nLung AD86.04\\n(82.01 - 89.25)87.90\\n(84.19 - 90.48)89.57\\n(87.30 - 90.93)94.41\\n(91.72 - 96.21)\\nGleason NCB80.79\\n(78.04 - 83.62)84.85\\n(82.42 - 87.37)89.14\\n(87.29 - 91.04)90.59\\n(88.73 - 92.45)\\nGleason RP85.43\\n(83.12 - 87.72)87.71\\n(85.30 - 89.93)89.39\\n(87.22 - 91.39)91.50\\n(89.77 - 93.10)\\nBreast IC89.47\\n(88.45 - 90.45)91.66\\n(90.71 - 92.53)92.18\\n(91.24 - 93.07)94.32\\n(93.52 - 95.07)\\nBreast NP68.75\\n(66.66 - 70.73)74.68\\n(72.70 - 76.59)73.54\\n(71.32 - 75.71)75.78\\n(73.50 - 77.90)\\nBreast TF74.49\\n(72.41 - 76.52)78.25\\n(76.25 - 80.15)75.66\\n(73.71 - 77.58)83.34\\n(81.64 - 84.99)\\nCIN86.17\\n(84.49 - 87.77)88.22\\n(86.58 - 89.79)89.49\\n(87.93 - 91.03)89.70\\n(87.96 - 91.30)\\nCRC97.29\\n(96.03 - 98.30)98.45\\n(97.42 - 99.22)98.54\\n(97.70 - 99.22)98.95\\n(98.02 - 99.60)\\nTCGA Study Type86.65\\n(85.53 - 87.75)91.85\\n(91.04 - 92.60)92.30\\n(91.47 - 93.05)96.38\\n(95.83 - 96.88)\\nTissue Type89.37\\n(88.50 - 90.19)92.25\\n(91.51 - 92.94)92.70\\n(92.00 - 93.34)94.76\\n(94.16 - 95.30)\\n59', metadata={'source': 'data\\\\med.pdf', 'page': 58}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nA.3. Evaluation metrics\\nBeyond human and expert evaluation, we leverage a range of automated metrics tailored to specific\\ntasks. For classification tasks, this may include basic accuracy and AUC (Area Under the ROC Curve)\\nmetrics. For tasks like report generation, where the fidelity and informativeness of the generated text\\nare crucial, we employ wide variety of metrics such as BLEU, Rouge-L or RadGraph F1-score to probe\\nthe quality of our models.\\nAccuracy Usedforimageclassificationandclose-endedVQAinferencetasks. Measuresthepercentage\\nof correct predictions vs.the ground truth.\\nAUC (Area Under the ROC Curve) AUC is a performance metric for classification models that\\nindicates how well a model distinguishes between different classes. AUC is calculated by plotting the\\nTrue Positive Rate (TPR) against the False Positive Rate (FPR) at various classification thresholds.\\nThe TPR measures the proportion of correctly identified positive instances, while the FPR measures\\nthe proportion of incorrectly identified negative instances. The area under this curve represents the\\nmodel’s overall ability to separate classes. An AUC of 1.0 indicates a perfect classifier, while an AUC\\nof 0.5 implies the model has no better discriminative power than random guessing.\\nF1 Score The F1 score is a valuable metric for evaluating classification models, especially when\\ndealing with imbalanced datasets. F1 score is calculated as the harmonic mean of precision (the\\nproportion of true positive out of all predicted positives) and recall (the proportion of true positives\\ncorrectly identified). The Weighted F1 Score which is used for VQA, averaging F1 scores across classes\\nbased on their frequency. Macro-F1 score used for image classification averaging F1 scores across\\nclasses without considering imbalances.\\nSensitivity Used for image classification in ophthalmology related tasks. Measures the percentage\\nof correctly identified positive cases out of all actual positive cases. A model with high sensitivity\\nminimizes false negatives.\\nSpecificity Used for image classification in ophthalmology related tasks. Measures the percentage\\nof correctly identified negative cases out of all actual negative cases. A model with high specificity\\nminimizes false positives.\\nTokenized F1 Tokenized F1-score provides a granular evaluation of language models by calculating\\nprecision, recall, and F1-score at the individual token level. This means it rewards partial matches,\\nrecognizing the model’s ability to identify elements within a sequence even if they’re not perfectly\\naligned. For this purpose True positives and false positives are determined as the number of correctly\\ngenerated tokens and tokens generated but not present in the ground truth, respectively. False\\nnegatives are tokens present in the ground truth but missed by the model.\\nRouge-L Rouge-L measures evaluates the quality of generated text and text summarization by\\ncomparing the longest common subsequence (LCS) between generated and reference text (Lin,\\n2004). Higher scores indicate better content and better salient point capturing. Rouge-L assesses the\\nsimilarity between generated and reference text by measuring the overlap of their LCS and calculating\\nrecall based on the LCS length relative to the reference text This metric takes into account the order\\nof words in the text, which makes it particularly suitable for evaluating summaries or text generation\\ntasks where the order of words matters. The higher the Rouge-L score, the better the quality of the\\ngenerated text compared to the reference text. ROUGE-L relies heavily on LCS and exact matches\\nlimiting the contextual understating of the generated text and increasing the sensitivity to sentence\\nlength. A high ROUGE-L score doesn’t necessarily ensure that the generated text is grammatically\\ncorrect, well-structured, or reads naturally.\\n60', metadata={'source': 'data\\\\med.pdf', 'page': 59}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nCIDErCIDEr (Consensus-based Image Description Evaluation) is a metric specifically designed to\\nassess the quality of captions generated for images and short text passages. It goes beyond simple\\nword overlap by considering both the n-gram matches (sequences of consecutive words) and the\\nimportance of those n-grams (Vedantam et al., 2015). In the preprocessing, both generated and\\nreference texts are converted to lowercase and common stop words (“the”, “a”, “an”) are removed.\\nWords are also stemmed, reducing them to their root form (e.g., “running” becomes “run”). Then\\nevery generated text is broken down into a series of n-grams which are sequences of ‘n’ consecutive\\nwords. A weight is assigned to each n-gram based on its Term Frequency-Inverse Document Frequency\\n(TF-IDF). This means common n-grams across all texts receive lower weights, while those that are\\nmore informative and distinctive get higher weights. The cosine similarity is calculated between the\\nTF-IDF weighted n-gram vectors of the generated text and each reference. The individual similarity\\nscores are averaged to produce the final CIDEr score. CIDEr can struggle to recognize texts that are\\nsemantically similar but use different synonyms and suffer from limited contextual understanding.\\nBLEU score The BLEU (Bilingual Evaluation Understudy) score is a widely used metric for evaluating\\nthe quality of AI generated text. It essentially compares a generated text to a set of human-written\\nreference, providing a score that indicates how similar they are (Papineni et al., 2002). BLEU focuses\\non n-gram precision, meaning it checks how often sequences of n consecutive words in the generated\\ntext appear in any of the reference. It also considers a brevity penalty to discourage generations\\nthat are significantly shorter than the reference text. Higher BLEU scores indicate better translation\\nquality, with a perfect score of 1.0 signifying a perfect match between the generated text and the\\nreference. BLEU score has limitations including lack of penalization for grammatical correctness,\\nfluency, or semantic equivalence. Additionally, the quality of the reference and ground truth can\\nimpact the BLEU score.\\nRadGraph F1-score RadGraph F1-score (Jain et al., 2021) is a performance metric specifically\\ndesignedtoevaluatetheaccuracyofmodelsthatextractstructuredmedicalinformationfromradiology\\nreports. Unlike standard F1-scores, RadGraph F1-score considers not only whether a finding is\\ncorrectly identified but also the accuracy of its relationships with other findings within the report.\\nThis is crucial because radiology reports often describe complex relationships between abnormalities,\\nlocations, and other attributes. Although RadGraph F1-score has its shortcomings, in comparison to\\nother automated NLG metrics provides a more holistic assessment of a model’s ability to understand\\nthe nuanced information present in free-text radiology reports.\\nWhile RadGraph F1-score offers a more nuanced evaluation than standard F1-scores for radiology\\nreport analysis, it has potential limitations. First, it relies on accurate RadGraph creation from the\\noriginal text. Errors in entity extraction or relation identification during this pre-processing stage\\ncould cascade into the RadGraph F1-score calculation. Secondly, it might be overly strict for partial\\nmatches and slight discrepancies in relationships or minor variations in wording could significantly\\npenalize the score. Finally, it may not fully account for the clinical relevance of certain errors, treating\\nall mismatches equally despite the potential for varying real-world impact.\\nTo compute the RadGraph F1-score, the model’s predictions on chest X-ray images are compared\\nagainst ground-truth report made by radiologists or other experts. To increase robustness of our\\ncalculationtoslightformatchanges, beforepassingtheground-truthandthegeneratedreportthrough\\nthe RadGraph F1-score package (Yu et al., 2023), we normalize both free-form text to lowercase.\\nThe F1-score takes into account both false positives (cases where the model incorrectly identifies\\nan abnormality) and false negatives (cases where the model fails to detect a true abnormality). By\\nconsidering both precision (the ratio of true positives to the total number of predicted positives)\\nand recall (the ratio of true positives to the total number of actual positives), the F1-score provides\\na balanced assessment of the model’s performance. A higher RadGraph F1-score indicates better\\nperformance in accurately identifying abnormalities in medical images, which is crucial for assisting\\nradiologists in diagnosis and treatment planning.\\n61', metadata={'source': 'data\\\\med.pdf', 'page': 60}),\n",
       " Document(page_content='Advancing Multimodal Medical Capabilities of Gemini\\nA.4. Supplementary Table for Performance Summary\\nTable A.15 presents the aggregate performance of Med-Gemini compared to the previous state-of-the-\\nart (SoTA), or a strong baseline where available. Figure 1 illustrates the relative improvement gained\\nby using one of our Med-Gemini models over the SoTA or strong baseline, using Gemini as a reference\\npoint when no SoTA is available. For pathology classification, we averaged AUC performance across\\nall sub-datasets. For report generation, we calculated the micro average performance across normal\\nand abnormal cases, expert identified “AI generated report is superior or similar to original report\"\\n(see Table 7)\\nTable A.15|Overall Performance Summary of Med-Gemini This table represent the aggregated results\\ncomparing Med-Gemini to the previous state-of-the-art (SoTA), Gemini or strong baseline where available.\\nCapabilities Datasets Metric Med-Gemini GeminiBaseline or\\nSoTAReference\\nReport\\nGenerationMIMIC-CXR RadGraph 24.4 N/A 20.5 Tanno et al. (2024)\\nMIMIC-CXRExpert\\n(AI superior)47.6 N/A 43.0 Tanno et al. (2024)\\nIND1Expert\\n(AI superior)75.4 N/A 63.7 Tanno et al. (2024)\\nMIMIC-CXR VQA Accuracy 78.6 70.9 68.1 Xu et al. (2023)\\nSlake-VQA Accuracy 84.8 70.4 91.1 Li et al. (2023b)\\nVQA-Rad CXR Expert 71.9 N/A 57.9 Xu et al. (2023)\\nVQA-Rad CXR Accuracy 78.8 62.4 N/A N/AVQA\\nPathVQA Accuracy 83.3 62.8 90.9 Sun et al. (2024)\\nGenomic\\nDisease\\nPredictionCoronary artery disease AUC 82.5 N/A 78.5\\nEnsemble of PRSs\\nand DemographicsStroke AUC 78.1 N/A 74.8\\nType 2 diabetes AUC 86.5 N/A 85.5\\nGlaucoma AUC 78.9 N/A 82.8\\nCOPD AUC 83.9 N/A 77.4\\nRheumatoid arthritis AUC 79.2 N/A 70.6\\nMajor depression AUC 70.2 N/A 60.2\\nAllcause mortality AUC 79.9 N/A 71.8\\nHypertension AUC 72.0 N/A 69.9\\nHypercholesterolemia AUC 52.4 N/A 52.1\\nAtrial fibrillation AUC 78.7 N/A 74.4\\nDiabetic retinopathy AUC 87.0 N/A 87.5\\nPneumonia AUC 71.6 N/A 68.9\\nAsthma AUC 57.7 N/A 56.5\\nHard Exudates F1 87.3 61.5 N/A N/A\\nHemorrhage F1 85.3 57.8 N/A N/A\\nPRP Scars F1 82.3 57.0 N/A N/A\\nDR Lesion F1 86.4 63.9 92.0 Krause et al. (2018)\\nPAD-UFES-20 F1 71.4 60.3 70.0 Google (2024)\\nMIMIC-CXR F1 90.7 86.8 N/A N/A\\nCheXpert F1 48.3 42.6 60.6 Tiu et al. (2022)\\nChestX-ray14 F1 46.7 34.2 58.3 Majkowska et al. (2020)Classification\\nPathology Patch AUC 89.2 88.4 91.7 Sun et al. (2024)\\n62', metadata={'source': 'data\\\\med.pdf', 'page': 61})]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the lines from the paper loaded line by line\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)  # PDF is of 62 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the documents into chunks since models have restrictions on the no of tokens\n",
    "\n",
    "def chunk_data(docs, chunk_size=550, chunk_overlap=60):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    doc = text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = chunk_data(docs=data)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(model='models/embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None, request_options=None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Vector Embeddings of the documents using GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=os.environ['GOOGLE_API_KEY'])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the embeddings\n",
    "vectors = embeddings.embed_query(\"Atmik Shetty!!!\")\n",
    "len(vectors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "# Creating a vector store for the embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(data, embedding = embeddings)   \n",
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='Bethesda, MD, USA , 2019.\\n41', metadata={'source': 'data\\\\med.pdf', 'page': 40}),\n",
       " 0.19530368)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Abstract of this research paper\"\n",
    "ans = db.similarity_search_with_score(query)\n",
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='Bethesda, MD, USA , 2019.\\n41', metadata={'source': 'data\\\\med.pdf', 'page': 40}),\n",
       " 0.19530368)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector = embeddings.embed_query(query)\n",
    "score = db.similarity_search_with_score_by_vector(embedding_vector)\n",
    "score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing from the DB\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.2)\n",
    "chain = load_qa_chain(model, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_answers(query):\n",
    "    doc_search = db.as_retriever()\n",
    "    ans = doc_search.invoke(query)\n",
    "    print(ans)\n",
    "    response = chain.run(input_documents = doc_search, question = query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = \"What Large langauge model is used in this research paper?\"\n",
    "answer = retrieve_answers(my_query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
